# Data Structure & Organization

## ğŸ“ Complete File Structure

```
Clario-Complete-Backend/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mock-spapi/                    â† ACTING SP-API (System Integration)
â”‚   â”‚   â”œâ”€â”€ financial_events.csv       âœ… 254 records (Claims/Reimbursements)
â”‚   â”‚   â”œâ”€â”€ orders.csv                 âœ… 240 records (Orders)
â”‚   â”‚   â”œâ”€â”€ inventory.csv              âœ… 240 records (Inventory)
â”‚   â”‚   â”œâ”€â”€ fees.csv                   âœ… 40 records (Fees)
â”‚   â”‚   â”œâ”€â”€ shipments_returns.csv     âœ… Empty (can add later)
â”‚   â”‚   â””â”€â”€ README.md                  (Format documentation)
â”‚   â”‚
â”‚   â””â”€â”€ ml-training/                    â† ML Training Data
â”‚       â”œâ”€â”€ processed_claims.csv      âœ… 240 records (Features + Labels)
â”‚       â”œâ”€â”€ train.csv                  âœ… 70% oldest (168 records)
â”‚       â”œâ”€â”€ val.csv                    âœ… 15% middle (36 records)
â”‚       â”œâ”€â”€ test.csv                   âœ… 15% newest (36 records)
â”‚       â””â”€â”€ summary.json               âœ… Metadata
â”‚
â””â”€â”€ raw_spapi_data.json                â† Original source (keep for reference)
```

---

## ğŸ¯ Purpose of Each Directory

### **`data/mock-spapi/` - ACTING SP-API**
**Purpose:** Simulates Amazon SP-API responses for system integration

**Files:**
- **`financial_events.csv`** â†’ Powers claims/reimbursements sync
- **`orders.csv`** â†’ Powers orders sync
- **`inventory.csv`** â†’ Powers inventory sync
- **`fees.csv`** â†’ Powers fees sync
- **`shipments_returns.csv`** â†’ Powers shipments/returns sync (empty for now)

**Flow:**
```
CSV Files â†’ Mock SP-API Service â†’ AmazonService â†’ Normalization â†’ Database â†’ Detection
```

**Usage:**
- Set `USE_MOCK_SPAPI=true`
- System reads CSV files as if they're real SP-API responses
- No real Amazon API calls needed

---

### **`data/ml-training/` - ML Training Data**
**Purpose:** Preprocessed data for machine learning model training

**Files:**
- **`processed_claims.csv`** â†’ Full dataset with engineered features + labels
- **`train.csv`** â†’ Training set (70% oldest, chronological)
- **`val.csv`** â†’ Validation set (15% middle)
- **`test.csv`** â†’ Test set (15% newest)
- **`summary.json`** â†’ Dataset metadata

**Flow:**
```
Processed CSV â†’ Feature Engineering â†’ Model Training â†’ Model Evaluation â†’ Model Deployment
```

**Usage:**
- Feed directly into ML training pipeline
- Already has features engineered and labels assigned
- Chronologically split for time-series validation

---

## ğŸ“Š Data Statistics

### Mock SP-API Data (System Integration)
- **Financial Events:** 254 records
- **Orders:** 240 records
- **Inventory:** 240 records
- **Fees:** 40 records
- **Shipments/Returns:** 0 records (empty)

### ML Training Data
- **Total Claims:** 240 records
- **Claimable (1):** 203 records (84.6%)
- **Not Claimable (0):** 37 records (15.4%)
- **Date Range:** 2024-01-07 to 2025-10-20
- **Train/Val/Test Split:** 70% / 15% / 15% (chronological)

---

## ğŸ”„ Data Flow Architecture

### **Phase 1: Data Ingestion (Mock SP-API)**
```
raw_spapi_data.json
    â†“ [Conversion Script]
data/mock-spapi/*.csv
    â†“ [Mock SP-API Service]
AmazonService (normalization)
    â†“
Database (claims, orders, inventory_items, financial_events)
```

### **Phase 2: ML Training**
```
data/ml-training/processed_claims.csv
    â†“ [Feature Engineering - Already Done]
train.csv, val.csv, test.csv
    â†“ [ML Training Pipeline]
Trained Models
    â†“
Model Deployment
```

---

## âœ… Ready to Go Checklist

- [x] **Mock SP-API CSV files created** (5 files in `data/mock-spapi/`)
- [x] **ML training data organized** (4 files in `data/ml-training/`)
- [x] **Conversion script working** (converts JSON â†’ CSV)
- [x] **File structure documented**
- [ ] **Environment variable set** (`USE_MOCK_SPAPI=true`)
- [ ] **Backend restarted**
- [ ] **Sync triggered** (to test data ingestion)

---

## ğŸš€ Next Steps

1. **Set Environment Variable:**
   ```bash
   # In Integrations-backend/.env or environment
   USE_MOCK_SPAPI=true
   ```

2. **Restart Backend:**
   ```bash
   cd Integrations-backend
   npm run dev
   ```

3. **Trigger Sync:**
   - System will read from `data/mock-spapi/*.csv`
   - Data flows: CSV â†’ Normalization â†’ Database â†’ Detection

4. **Train ML Models:**
   - Use `data/ml-training/train.csv`, `val.csv`, `test.csv`
   - Models train on your synthetic data
   - Deploy trained models

---

## ğŸ“ Notes

- **Mock SP-API files** are for system integration (sync, detection, database)
- **ML training files** are for model training (separate pipeline)
- Both use the same source data (`raw_spapi_data.json`) but serve different purposes
- Mock SP-API files are in SP-API format (for system compatibility)
- ML training files have engineered features (for model training)

