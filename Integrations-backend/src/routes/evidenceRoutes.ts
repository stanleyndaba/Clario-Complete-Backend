/**
 * Evidence Routes
 * Handles evidence ingestion and document management
 */

import { Router, Request, Response } from 'express';
import multer from 'multer';
import { gmailIngestionService } from '../services/gmailIngestionService';
import { outlookIngestionService } from '../services/outlookIngestionService';
import { googleDriveIngestionService } from '../services/googleDriveIngestionService';
import { dropboxIngestionService } from '../services/dropboxIngestionService';
import { unifiedIngestionService } from '../services/unifiedIngestionService';
import { supabase, supabaseAdmin } from '../database/supabaseClient';
import logger from '../utils/logger';

// Type for multer file
interface MulterFile {
  fieldname: string;
  originalname: string;
  encoding: string;
  mimetype: string;
  buffer: Buffer;
  size: number;
}

const router = Router();

/**
 * GET /api/evidence/sources
 * Get all connected evidence sources for the current user
 */
router.get('/sources', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id || 'demo-user';

    logger.info('ðŸ“‹ [EVIDENCE] Fetching evidence sources', { userId });

    // Try to get sources from evidence_sources table
    const { data: sources, error } = await supabaseAdmin
      .from('evidence_sources')
      .select('*')
      .or(`user_id.eq.${userId},seller_id.eq.${userId}`)
      .order('created_at', { ascending: false });

    if (error) {
      logger.warn('âš ï¸ [EVIDENCE] Error fetching evidence sources, checking integrations table', { error: error.message });

      // Fallback: Check integrations table for connected OAuth integrations
      const { data: integrations, error: intError } = await supabaseAdmin
        .from('integrations')
        .select('*')
        .or(`user_id.eq.${userId},seller_id.eq.${userId}`)
        .eq('status', 'connected');

      if (intError || !integrations) {
        // Return demo sources for testing
        return res.json({
          success: true,
          sources: [
            {
              id: 'demo-gmail',
              provider: 'gmail',
              account_email: 'demo@gmail.com',
              status: 'connected',
              last_sync_at: new Date().toISOString()
            }
          ]
        });
      }

      // Map integrations to sources format
      const mappedSources = integrations.map((int: any) => ({
        id: int.id,
        provider: int.provider,
        account_email: int.email || int.account_email || `${int.provider}@connected.local`,
        status: int.status || 'connected',
        last_sync_at: int.last_sync_at || int.updated_at
      }));

      return res.json({
        success: true,
        sources: mappedSources
      });
    }

    // Filter to connected sources only
    const connectedSources = (sources || []).filter((s: any) => s.status === 'connected' || s.is_active);

    // If no sources, provide demo source for testing
    if (connectedSources.length === 0) {
      return res.json({
        success: true,
        sources: [
          {
            id: 'demo-gmail',
            provider: 'gmail',
            account_email: 'demo@gmail.com',
            status: 'connected',
            last_sync_at: new Date().toISOString()
          }
        ],
        note: 'Using demo source for testing'
      });
    }

    res.json({
      success: true,
      sources: connectedSources.map((s: any) => ({
        id: s.id,
        provider: s.provider,
        account_email: s.account_email || s.email || `${s.provider}@connected.local`,
        status: s.status || 'connected',
        last_sync_at: s.last_sync_at || s.updated_at
      }))
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error fetching evidence sources', {
      error: error?.message || String(error)
    });

    // Return demo source on error for testing
    res.json({
      success: true,
      sources: [
        {
          id: 'demo-gmail',
          provider: 'gmail',
          account_email: 'demo@gmail.com',
          status: 'connected',
          last_sync_at: new Date().toISOString()
        }
      ],
      note: 'Using demo source (error fetching real sources)'
    });
  }
});

/**
 * POST /api/evidence/ingest/outlook
 * Trigger Outlook evidence ingestion
 */
router.post('/ingest/outlook', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    const { query, maxResults, autoParse } = req.body;

    logger.info('ðŸ” [EVIDENCE] Starting Outlook evidence ingestion', {
      userId,
      query,
      maxResults,
      autoParse
    });

    // Send SSE event for ingestion start
    try {
      const sseHub = (await import('../utils/sseHub')).default;
      sseHub.sendEvent(userId, 'evidence_ingestion_started', {
        userId,
        provider: 'outlook',
        timestamp: new Date().toISOString()
      });
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion start', { error: sseError });
    }

    const result = await outlookIngestionService.ingestEvidenceFromOutlook(userId, {
      query,
      maxResults: maxResults || 50,
      autoParse: autoParse !== false
    });

    // Send SSE event for ingestion completion
    try {
      const sseHub = (await import('../utils/sseHub')).default;
      sseHub.sendEvent(userId, 'evidence_ingestion_completed', {
        userId,
        provider: 'outlook',
        documentsIngested: result.documentsIngested,
        emailsProcessed: result.emailsProcessed,
        errors: result.errors.length,
        timestamp: new Date().toISOString()
      });
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion completion', { error: sseError });
    }

    res.json({
      success: result.success,
      documentsIngested: result.documentsIngested,
      emailsProcessed: result.emailsProcessed,
      errors: result.errors,
      message: `Ingested ${result.documentsIngested} documents from ${result.emailsProcessed} emails`
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in Outlook ingestion endpoint', {
      error: error?.message || String(error),
      stack: error?.stack
    });

    res.status(500).json({
      success: false,
      error: 'Failed to ingest evidence from Outlook',
      message: error?.message || String(error)
    });
  }
});

/**
 * POST /api/evidence/ingest/gdrive
 * Trigger Google Drive evidence ingestion
 */
router.post('/ingest/gdrive', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    const { query, maxResults, autoParse, folderId } = req.body;

    logger.info('ðŸ” [EVIDENCE] Starting Google Drive evidence ingestion', {
      userId,
      query,
      maxResults,
      autoParse,
      folderId
    });

    // Send SSE event for ingestion start
    try {
      const sseHub = (await import('../utils/sseHub')).default;
      sseHub.sendEvent(userId, 'evidence_ingestion_started', {
        userId,
        provider: 'gdrive',
        timestamp: new Date().toISOString()
      });
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion start', { error: sseError });
    }

    const result = await googleDriveIngestionService.ingestEvidenceFromGoogleDrive(userId, {
      query,
      maxResults: maxResults || 50,
      autoParse: autoParse !== false,
      folderId
    });

    // Send SSE event for ingestion completion
    try {
      const sseHub = (await import('../utils/sseHub')).default;
      sseHub.sendEvent(userId, 'evidence_ingestion_completed', {
        userId,
        provider: 'gdrive',
        documentsIngested: result.documentsIngested,
        filesProcessed: result.filesProcessed,
        errors: result.errors.length,
        timestamp: new Date().toISOString()
      });
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion completion', { error: sseError });
    }

    res.json({
      success: result.success,
      documentsIngested: result.documentsIngested,
      filesProcessed: result.filesProcessed,
      errors: result.errors,
      message: `Ingested ${result.documentsIngested} documents from ${result.filesProcessed} files`
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in Google Drive ingestion endpoint', {
      error: error?.message || String(error),
      stack: error?.stack
    });

    res.status(500).json({
      success: false,
      error: 'Failed to ingest evidence from Google Drive',
      message: error?.message || String(error)
    });
  }
});

/**
 * POST /api/evidence/ingest/dropbox
 * Trigger Dropbox evidence ingestion
 */
router.post('/ingest/dropbox', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    const { query, maxResults, autoParse, folderPath } = req.body;

    logger.info('ðŸ” [EVIDENCE] Starting Dropbox evidence ingestion', {
      userId,
      query,
      maxResults,
      autoParse,
      folderPath
    });

    // Send SSE event for ingestion start
    try {
      const sseHub = (await import('../utils/sseHub')).default;
      sseHub.sendEvent(userId, 'evidence_ingestion_started', {
        userId,
        provider: 'dropbox',
        timestamp: new Date().toISOString()
      });
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion start', { error: sseError });
    }

    const result = await dropboxIngestionService.ingestEvidenceFromDropbox(userId, {
      query,
      maxResults: maxResults || 50,
      autoParse: autoParse !== false,
      folderPath
    });

    // Send SSE event for ingestion completion
    try {
      const sseHub = (await import('../utils/sseHub')).default;
      sseHub.sendEvent(userId, 'evidence_ingestion_completed', {
        userId,
        provider: 'dropbox',
        documentsIngested: result.documentsIngested,
        filesProcessed: result.filesProcessed,
        errors: result.errors.length,
        timestamp: new Date().toISOString()
      });
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion completion', { error: sseError });
    }

    res.json({
      success: result.success,
      documentsIngested: result.documentsIngested,
      filesProcessed: result.filesProcessed,
      errors: result.errors,
      message: `Ingested ${result.documentsIngested} documents from ${result.filesProcessed} files`
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in Dropbox ingestion endpoint', {
      error: error?.message || String(error),
      stack: error?.stack
    });

    res.status(500).json({
      success: false,
      error: 'Failed to ingest evidence from Dropbox',
      message: error?.message || String(error)
    });
  }
});

/**
 * POST /api/evidence/ingest/all
 * Trigger unified evidence ingestion from all connected sources
 */
router.post('/ingest/all', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    const { providers, query, maxResults, autoParse, folderId, folderPath } = req.body;

    logger.info('ðŸ” [EVIDENCE] Starting unified evidence ingestion', {
      userId,
      providers,
      query,
      maxResults,
      autoParse
    });

    // Send SSE event for ingestion start
    try {
      const sseHub = (await import('../utils/sseHub')).default;
      sseHub.sendEvent(userId, 'evidence_ingestion_started', {
        userId,
        provider: 'all',
        providers: providers,
        timestamp: new Date().toISOString()
      });
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion start', { error: sseError });
    }

    const result = await unifiedIngestionService.ingestFromAllSources(userId, {
      providers,
      query,
      maxResults: maxResults || 50,
      autoParse: autoParse !== false,
      folderId,
      folderPath
    });

    // Send SSE event for ingestion completion
    try {
      const sseHub = (await import('../utils/sseHub')).default;
      sseHub.sendEvent(userId, 'evidence_ingestion_completed', {
        userId,
        provider: 'all',
        totalDocumentsIngested: result.totalDocumentsIngested,
        totalItemsProcessed: result.totalItemsProcessed,
        errors: result.errors.length,
        timestamp: new Date().toISOString()
      });
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion completion', { error: sseError });
    }

    res.json({
      success: result.success,
      totalDocumentsIngested: result.totalDocumentsIngested,
      totalItemsProcessed: result.totalItemsProcessed,
      errors: result.errors,
      results: result.results,
      message: `Ingested ${result.totalDocumentsIngested} documents from ${result.totalItemsProcessed} items across all sources`
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in unified ingestion endpoint', {
      error: error?.message || String(error),
      stack: error?.stack
    });

    res.status(500).json({
      success: false,
      error: 'Failed to ingest evidence from all sources',
      message: error?.message || String(error)
    });
  }
});

/**
 * POST /api/evidence/ingest/gmail
 * Trigger Gmail evidence ingestion
 */
router.post('/ingest/gmail', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    const { query, maxResults, autoParse } = req.body;

    logger.info('ðŸ” [EVIDENCE] Starting Gmail evidence ingestion', {
      userId,
      query,
      maxResults,
      autoParse
    });

    // Send SSE event for ingestion start
    try {
      const sseHub = (await import('../utils/sseHub')).default;
      sseHub.sendEvent(userId, 'evidence_ingestion_started', {
        userId,
        timestamp: new Date().toISOString()
      });
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion start', { error: sseError });
    }

    const result = await gmailIngestionService.ingestEvidenceFromGmail(userId, {
      query,
      maxResults: maxResults || 50,
      autoParse: autoParse !== false // Default to true
    });

    // Send SSE event for ingestion completion
    try {
      const sseHub = (await import('../utils/sseHub')).default;
      sseHub.sendEvent(userId, 'evidence_ingestion_completed', {
        userId,
        documentsIngested: result.documentsIngested,
        emailsProcessed: result.emailsProcessed,
        errors: result.errors.length,
        timestamp: new Date().toISOString()
      });
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion completion', { error: sseError });
    }

    res.json({
      success: result.success,
      documentsIngested: result.documentsIngested,
      emailsProcessed: result.emailsProcessed,
      errors: result.errors,
      message: `Ingested ${result.documentsIngested} documents from ${result.emailsProcessed} emails`
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in Gmail ingestion endpoint', {
      error: error?.message || String(error),
      stack: error?.stack
    });

    // Send SSE event for ingestion error
    try {
      const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;
      if (userId) {
        const sseHub = (await import('../utils/sseHub')).default;
        sseHub.sendEvent(userId, 'evidence_ingestion_failed', {
          userId,
          error: error?.message || String(error),
          timestamp: new Date().toISOString()
        });
      }
    } catch (sseError) {
      logger.debug('Failed to send SSE event for ingestion error', { error: sseError });
    }

    res.status(500).json({
      success: false,
      error: 'Failed to ingest evidence from Gmail',
      message: error?.message || String(error)
    });
  }
});

/**
 * GET /api/evidence/status
 * Get evidence ingestion status
 */
router.get('/status', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    const status = await gmailIngestionService.getIngestionStatus(userId);

    res.json({
      success: true,
      ...status
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error getting ingestion status', {
      error: error?.message || String(error)
    });

    res.status(500).json({
      success: false,
      error: 'Failed to get ingestion status',
      message: error?.message || String(error)
    });
  }
});

/**
 * POST /api/evidence/auto-collect
 * Enable/disable auto-collect for evidence ingestion
 */
router.post('/auto-collect', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        ok: false,
        error: 'Unauthorized'
      });
    }

    const { enabled } = req.body;

    // Store auto-collect setting in database (evidence_sources metadata or user settings)
    try {
      const { supabase } = await import('../database/supabaseClient');
      // Update user settings or evidence source metadata
      // For now, just return success
      logger.info('Auto-collect setting updated', { userId, enabled });
    } catch (dbError) {
      logger.warn('Failed to update auto-collect setting', { error: dbError });
    }

    res.json({
      ok: true,
      enabled: enabled,
      message: `Auto-collect ${enabled ? 'enabled' : 'disabled'}`
    });
  } catch (error: any) {
    logger.error('Error updating auto-collect setting', {
      error: error?.message || String(error)
    });

    res.status(500).json({
      ok: false,
      error: 'Failed to update auto-collect setting'
    });
  }
});

/**
 * POST /api/evidence/schedule
 * Set evidence ingestion schedule
 */
router.post('/schedule', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        ok: false,
        error: 'Unauthorized'
      });
    }

    const { schedule } = req.body;

    // Validate schedule
    const validSchedules = ['daily_0200', 'daily_1200', 'hourly', 'weekly'];
    if (schedule && !validSchedules.includes(schedule)) {
      return res.status(400).json({
        ok: false,
        error: `Invalid schedule. Valid schedules: ${validSchedules.join(', ')}`
      });
    }

    // Store schedule in database
    try {
      const { supabase } = await import('../database/supabaseClient');
      // Update user settings or evidence source metadata
      logger.info('Evidence ingestion schedule updated', { userId, schedule });
    } catch (dbError) {
      logger.warn('Failed to update schedule', { error: dbError });
    }

    res.json({
      ok: true,
      schedule: schedule,
      message: `Schedule set to ${schedule}`
    });
  } catch (error: any) {
    logger.error('Error updating schedule', {
      error: error?.message || String(error)
    });

    res.status(500).json({
      ok: false,
      error: 'Failed to update schedule'
    });
  }
});

/**
 * POST /api/evidence/filters
 * Set evidence ingestion filters
 */
router.post('/filters', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        ok: false,
        error: 'Unauthorized'
      });
    }

    const { includeSenders, excludeSenders, fileTypes, folders } = req.body;

    // Store filters in database (evidence_sources metadata)
    try {
      const { supabase } = await import('../database/supabaseClient');
      // Update evidence source metadata with filters
      const filters = {
        includeSenders: includeSenders || [],
        excludeSenders: excludeSenders || [],
        fileTypes: fileTypes || [],
        folders: folders || []
      };

      // Update all evidence sources for user
      await supabase
        .from('evidence_sources')
        .update({
          metadata: filters,
          updated_at: new Date().toISOString()
        })
        .eq('user_id', userId);

      logger.info('Evidence ingestion filters updated', { userId, filters });
    } catch (dbError) {
      logger.warn('Failed to update filters', { error: dbError });
    }

    res.json({
      ok: true,
      filters: {
        includeSenders: includeSenders || [],
        excludeSenders: excludeSenders || [],
        fileTypes: fileTypes || [],
        folders: folders || []
      },
      message: 'Filters updated successfully'
    });
  } catch (error: any) {
    logger.error('Error updating filters', {
      error: error?.message || String(error)
    });

    res.status(500).json({
      ok: false,
      error: 'Failed to update filters'
    });
  }
});

/**
 * POST /api/evidence/upload
 * Fallback endpoint for document upload - uses multer to handle files and proxies to Python API
 * This endpoint handles file uploads and forwards them to the Python backend
 */

// Configure multer for file uploads (memory storage)
const uploadStorage = multer.memoryStorage();
const uploadMulter = multer({
  storage: uploadStorage,
  limits: {
    fileSize: 50 * 1024 * 1024 // 50MB limit (frontend shows 10MB, but backend allows up to 50MB)
  }
});

// Handle CORS preflight for upload endpoint
router.options('/upload', (req, res) => {
  const origin = req.headers.origin;
  logger.debug('CORS preflight for /api/evidence/upload', { origin });
  res.header('Access-Control-Allow-Origin', origin || '*');
  res.header('Access-Control-Allow-Credentials', 'true');
  res.header('Access-Control-Allow-Methods', 'POST, OPTIONS');
  res.header('Access-Control-Allow-Headers', 'Content-Type, Authorization, X-Requested-With, X-User-Id, X-Forwarded-User-Id, Origin, Referer');
  res.header('Access-Control-Max-Age', '86400');
  res.status(204).send();
});

router.post('/upload', uploadMulter.any(), async (req: Request, res: Response) => {
  try {
    // Extract user ID from multiple sources (for flexibility)
    const userId = (req as any).userId ||
      (req as any).user?.id ||
      (req as any).user?.user_id ||
      req.headers['x-user-id'] ||
      req.headers['x-forwarded-user-id'] ||
      req.query.userId as string;

    // Log all available user identification sources for debugging
    logger.debug('ðŸ” [EVIDENCE] User ID extraction', {
      'req.userId': (req as any).userId,
      'req.user.id': (req as any).user?.id,
      'req.user.user_id': (req as any).user?.user_id,
      'x-user-id': req.headers['x-user-id'],
      'x-forwarded-user-id': req.headers['x-forwarded-user-id'],
      'query.userId': req.query.userId,
      'extractedUserId': userId,
      'hasAuthHeader': !!req.headers['authorization'],
      'hasCookie': !!req.cookies?.session_token
    });

    // Allow demo-user for development/testing (userIdMiddleware sets this as default)
    // In production, authentication middleware should set a real user ID
    const finalUserId = userId || 'demo-user';

    if (!userId) {
      logger.warn('âš ï¸ [EVIDENCE] Upload request without user ID - using demo-user fallback', {
        headers: {
          'x-user-id': req.headers['x-user-id'],
          'authorization': req.headers['authorization'] ? 'present' : 'missing',
          'cookie': req.cookies ? 'present' : 'missing'
        },
        path: req.path,
        method: req.method,
        note: 'This is OK for development/testing. In production, ensure authentication middleware sets user ID.'
      });
    }

    const files = (req as any).files as MulterFile[];
    const claim_id = req.query.claim_id as string | undefined;

    if (!files || files.length === 0) {
      return res.status(400).json({
        success: false,
        error: 'No files provided',
        message: 'Expected at least one file in the request'
      });
    }

    logger.info('ðŸ“¤ [EVIDENCE] Document upload request received', {
      userId,
      fileCount: files.length,
      filenames: files.map(f => f.originalname),
      claim_id
    });

    // Proxy to Python API /api/documents/upload endpoint
    const pythonApiUrl = process.env.PYTHON_API_URL || process.env.VITE_PYTHON_API_URL || 'https://python-api-10.onrender.com';
    const pythonUrl = `${pythonApiUrl}/api/documents/upload${claim_id ? `?claim_id=${claim_id}` : ''}`;

    // DEMO MODE: Check if Python API should be skipped (for YC demo)
    const SKIP_PYTHON_API = process.env.SKIP_PYTHON_API === 'true';
    const DEMO_MODE = process.env.DEMO_MODE === 'true';

    // Helper function to return mock success response
    const returnMockResponse = () => {
      const origin = req.headers.origin;
      if (origin) {
        res.header('Access-Control-Allow-Origin', origin);
        res.header('Access-Control-Allow-Credentials', 'true');
      }

      const mockDocumentIds = files.map((_, index) => `demo-doc-${Date.now()}-${index}`);

      // Send SSE event for upload success (mock)
      (async () => {
        try {
          const sseHub = (await import('../utils/sseHub')).default;
          sseHub.sendEvent(finalUserId, 'evidence_upload_completed', {
            userId: finalUserId,
            documentId: mockDocumentIds[0],
            documentIds: mockDocumentIds,
            status: 'uploaded',
            processingStatus: 'pending',
            fileCount: files.length,
            message: 'Document uploaded successfully (DEMO MODE)',
            timestamp: new Date().toISOString(),
            demoMode: true
          });
        } catch (sseError) {
          logger.debug('Failed to send SSE event for mock upload', { error: sseError });
        }
      })();

      return res.json({
        success: true,
        id: mockDocumentIds[0],
        document_ids: mockDocumentIds,
        status: 'uploaded',
        processing_status: 'pending',
        file_count: files.length,
        uploaded_at: new Date().toISOString(),
        message: `Documents uploaded successfully (DEMO MODE - ${files.length} file(s))`,
        demoMode: true,
        note: 'Python API is unavailable. This is a mock response for demo purposes.'
      });
    };

    // If demo mode is enabled, return mock response immediately
    if (SKIP_PYTHON_API || DEMO_MODE) {
      logger.info('ðŸŽ­ [EVIDENCE] DEMO MODE: Returning mock response (Python API skipped)', {
        userId: finalUserId,
        fileCount: files.length,
        filenames: files.map(f => f.originalname),
        skipPythonApi: SKIP_PYTHON_API,
        demoMode: DEMO_MODE
      });
      return returnMockResponse();
    }

    // Extract token for authentication
    const token = req.cookies?.session_token || req.headers['authorization']?.replace('Bearer ', '');

    logger.info('ðŸ“¤ [EVIDENCE] Starting upload to Python API', {
      pythonApiUrl,
      pythonUrl,
      userId: finalUserId,
      fileCount: files.length,
      filenames: files.map(f => f.originalname),
      claim_id,
      hasToken: !!token
    });

    try {
      const axios = (await import('axios')).default;
      const FormData = (await import('form-data')).default;

      // Quick health check before attempting upload (reduced timeout for faster failure detection)
      try {
        const healthCheckUrl = `${pythonApiUrl}/health`;
        logger.debug('ðŸ” [EVIDENCE] Checking Python API health before upload', { healthCheckUrl });

        const healthResponse = await axios.get(healthCheckUrl, {
          timeout: 3000, // Reduced to 3 seconds for faster failure detection
          validateStatus: (status) => status < 500 // Allow 4xx but not 5xx
        }).catch((healthError: any) => {
          // Health check failed - use demo mode fallback for YC demo
          logger.warn('âš ï¸ [EVIDENCE] Python API health check failed - using demo mode fallback', {
            error: healthError?.message,
            code: healthError?.code,
            status: healthError?.response?.status,
            note: 'Returning mock response for YC demo'
          });

          // Return mock response instead of error (for YC demo)
          return null; // Signal that health check failed
        });

        if (!healthResponse) {
          // Health check failed - use demo mode fallback
          return returnMockResponse();
        }

        const healthStatus = healthResponse.status;
        if (healthStatus === 200) {
          logger.info('âœ… [EVIDENCE] Python API health check passed', {
            status: healthStatus,
            data: healthResponse.data
          });
        } else if (healthStatus >= 500) {
          // Python API is down - use demo mode fallback for YC demo
          logger.warn('âš ï¸ [EVIDENCE] Python API health check indicates service is down - using demo mode fallback', {
            status: healthStatus,
            data: healthResponse.data,
            note: 'Returning mock response for YC demo'
          });

          // Return mock response instead of error (for YC demo)
          return returnMockResponse();
        }
      } catch (healthError: any) {
        // Health check error - use demo mode fallback for YC demo
        logger.warn('âš ï¸ [EVIDENCE] Python API health check error - using demo mode fallback', {
          error: healthError?.message,
          code: healthError?.code,
          note: 'Returning mock response for YC demo'
        });

        // Return mock response instead of error (for YC demo)
        return returnMockResponse();
      }

      // Create FormData to forward files
      const formData = new FormData();

      // Add all files with 'file' field name (singular, as expected by Python API)
      files.forEach(file => {
        formData.append('file', file.buffer, {
          filename: file.originalname,
          contentType: file.mimetype
        });
      });

      // Add claim_id if provided (as form field, not query param for POST)
      if (claim_id) {
        formData.append('claim_id', claim_id);
      }

      const headers: Record<string, string> = {
        ...formData.getHeaders(),
        'X-User-Id': userId
      };

      if (token) {
        headers['Authorization'] = `Bearer ${token}`;
      }

      logger.info('ðŸ“¤ [EVIDENCE] Forwarding upload to Python API', {
        pythonUrl,
        userId,
        fileCount: files.length,
        headers: Object.keys(headers)
      });

      // Make request with extended timeout and better error handling
      const response = await axios.post(pythonUrl, formData, {
        headers,
        timeout: 120000, // 120 second timeout for file uploads (increased)
        maxContentLength: Infinity,
        maxBodyLength: Infinity,
        validateStatus: (status) => status < 500, // Don't throw on 4xx errors
        maxRedirects: 5
      }).catch((error: any) => {
        // Enhanced error logging
        const responseData = error?.response?.data;
        const isHtmlError = typeof responseData === 'string' && (
          responseData.trim().startsWith('<!DOCTYPE') ||
          responseData.trim().startsWith('<html')
        );

        logger.error('âŒ [EVIDENCE] Axios error details', {
          message: error?.message,
          code: error?.code,
          status: error?.response?.status,
          statusText: error?.response?.statusText,
          isHtmlError: isHtmlError,
          data: isHtmlError ? 'HTML error page (service down)' : responseData,
          contentType: error?.response?.headers?.['content-type'],
          config: {
            url: error?.config?.url,
            method: error?.config?.method,
            timeout: error?.config?.timeout
          },
          stack: error?.stack
        });

        // If it's an HTML error page, wrap it in a more useful error
        if (isHtmlError && error?.response?.status === 502) {
          const serviceDownError: any = new Error('Python API service unavailable (502)');
          serviceDownError.code = 'PYTHON_API_DOWN';
          serviceDownError.response = {
            status: 503,
            data: {
              success: false,
              error: 'Service unavailable',
              message: 'The document processing service is currently unavailable. Please try again in a few moments.',
              code: 'PYTHON_API_DOWN'
            }
          };
          throw serviceDownError;
        }

        throw error;
      });

      // Check if response is successful and valid JSON
      if (response.status >= 400) {
        // Check if response is HTML (Render error page)
        const contentType = response.headers['content-type'] || '';
        const isHtml = typeof response.data === 'string' && (
          response.data.trim().startsWith('<!DOCTYPE') ||
          response.data.trim().startsWith('<html') ||
          contentType.includes('text/html')
        );

        if (isHtml) {
          logger.error('âŒ [EVIDENCE] Python API returned HTML error page (service may be down)', {
            status: response.status,
            pythonApiUrl,
            pythonUrl,
            userId,
            responsePreview: typeof response.data === 'string' ? response.data.substring(0, 200) : 'N/A'
          });

          // Send SSE event for upload error
          try {
            const sseHub = (await import('../utils/sseHub')).default;
            sseHub.sendEvent(userId, 'evidence_upload_failed', {
              userId,
              error: 'Python API service unavailable',
              statusCode: 503,
              message: 'The document processing service is currently unavailable. Please try again later.',
              timestamp: new Date().toISOString()
            });
          } catch (sseError) {
            logger.debug('Failed to send SSE event for upload error', { error: sseError });
          }

          return res.status(503).json({
            success: false,
            error: 'Service unavailable',
            message: 'The document processing service is currently unavailable. Please try again in a few moments.',
            code: 'PYTHON_API_DOWN',
            retryAfter: 60
          });
        }

        logger.error('âŒ [EVIDENCE] Python API returned error status', {
          status: response.status,
          statusText: response.statusText,
          data: response.data,
          userId
        });

        // Send SSE event for upload error
        try {
          const sseHub = (await import('../utils/sseHub')).default;
          sseHub.sendEvent(userId, 'evidence_upload_failed', {
            userId,
            error: response.data?.error || response.data?.detail || `HTTP ${response.status}`,
            statusCode: response.status,
            errorDetails: response.data,
            timestamp: new Date().toISOString()
          });
        } catch (sseError) {
          logger.debug('Failed to send SSE event for upload error', { error: sseError });
        }

        return res.status(response.status).json({
          success: false,
          error: response.data?.error || response.data?.detail || 'Upload failed',
          message: response.data?.message || `Python API returned status ${response.status}`,
          details: response.data
        });
      }

      logger.info('âœ… [EVIDENCE] Document upload successful', {
        userId,
        documentId: response.data.id,
        status: response.data.status,
        responseStatus: response.status
      });

      // Send SSE event for upload success
      try {
        const sseHub = (await import('../utils/sseHub')).default;
        sseHub.sendEvent(userId, 'evidence_upload_completed', {
          userId,
          documentId: response.data.id || response.data.document_ids?.[0],
          documentIds: response.data.document_ids || [response.data.id],
          status: response.data.status || 'uploaded',
          processingStatus: response.data.processing_status || 'processing',
          fileCount: response.data.file_count || 1,
          message: response.data.message || 'Document uploaded successfully',
          timestamp: new Date().toISOString()
        });

        // Also send parsing_started event if processing_status is 'processing'
        if (response.data.processing_status === 'processing') {
          sseHub.sendEvent(userId, 'parsing_started', {
            userId,
            documentId: response.data.id || response.data.document_ids?.[0],
            timestamp: new Date().toISOString()
          });
        }
      } catch (sseError) {
        logger.debug('Failed to send SSE event for upload completion', { error: sseError });
      }

      // Return success response
      return res.json({
        success: true,
        ...response.data
      });
    } catch (proxyError: any) {
      // Enhanced error handling
      const errorMessage = proxyError?.message || String(proxyError);
      const errorCode = proxyError?.code;
      const responseStatus = proxyError?.response?.status;
      const responseData = proxyError?.response?.data;

      logger.error('âŒ [EVIDENCE] Error forwarding upload to Python API', {
        error: errorMessage,
        code: errorCode,
        status: responseStatus,
        data: responseData,
        pythonUrl,
        userId,
        fileCount: files.length,
        stack: proxyError?.stack
      });

      // Determine appropriate error response
      let statusCode = 502;
      let errorResponse: any = {
        success: false,
        error: 'Failed to upload documents',
        message: 'Python API unavailable or error occurred'
      };

      if (proxyError.response) {
        // Python API responded with an error
        statusCode = proxyError.response.status;
        errorResponse = {
          success: false,
          error: responseData?.error || responseData?.detail || 'Upload failed',
          message: responseData?.message || `Python API returned status ${statusCode}`,
          details: responseData
        };
      } else if (errorCode === 'PYTHON_API_DOWN' || (responseStatus === 502 && typeof responseData === 'string' && responseData.includes('<!DOCTYPE'))) {
        // Python API is down (502 with HTML response)
        statusCode = 503;
        errorResponse = {
          success: false,
          error: 'Service unavailable',
          message: 'The document processing service is currently unavailable. Please try again in a few moments.',
          code: 'PYTHON_API_DOWN',
          retryAfter: 60
        };
      } else if (errorCode === 'ECONNREFUSED' || errorCode === 'ETIMEDOUT' || errorCode === 'ENOTFOUND') {
        // Connection issues - use demo mode fallback for demo purposes
        logger.warn('ðŸŽ­ [EVIDENCE] Python API connection failed - using demo mode fallback', {
          errorCode,
          pythonApiUrl,
          userId: finalUserId
        });

        // Set CORS headers
        const origin = req.headers.origin;
        if (origin) {
          res.header('Access-Control-Allow-Origin', origin);
          res.header('Access-Control-Allow-Credentials', 'true');
        }

        // Generate mock document IDs
        const mockDocumentIds = files.map((_, index) => `demo-doc-${Date.now()}-${index}`);

        // Send SSE event for upload success (mock)
        try {
          const sseHub = (await import('../utils/sseHub')).default;
          sseHub.sendEvent(finalUserId, 'evidence_upload_completed', {
            userId: finalUserId,
            documentId: mockDocumentIds[0],
            documentIds: mockDocumentIds,
            status: 'uploaded',
            processingStatus: 'pending',
            fileCount: files.length,
            message: 'Document uploaded successfully (DEMO MODE - Python API connection failed)',
            timestamp: new Date().toISOString(),
            demoMode: true
          });
        } catch (sseError) {
          logger.debug('Failed to send SSE event for mock upload', { error: sseError });
        }

        // Return mock success response instead of error
        return res.json({
          success: true,
          id: mockDocumentIds[0],
          document_ids: mockDocumentIds,
          status: 'uploaded',
          processing_status: 'pending',
          file_count: files.length,
          uploaded_at: new Date().toISOString(),
          message: `Documents uploaded successfully (DEMO MODE - Python API unavailable, ${files.length} file(s))`,
          demoMode: true,
          note: 'Python API is currently unavailable. This is a mock response for demo purposes.'
        });
      } else if (errorCode === 'ECONNABORTED') {
        // Timeout
        statusCode = 504;
        errorResponse = {
          success: false,
          error: 'Upload timeout',
          message: 'The upload request timed out. The file may be too large or the server is busy. Please try again with a smaller file or wait a few moments.',
          code: errorCode,
          retryAfter: 30
        };
      } else {
        // Other errors
        statusCode = responseStatus || 502;
        errorResponse = {
          success: false,
          error: 'Upload failed',
          message: errorMessage || 'An unexpected error occurred during upload. Please try again.',
          code: errorCode || 'UNKNOWN_ERROR'
        };
      }

      // Send SSE event for upload error
      try {
        const sseHub = (await import('../utils/sseHub')).default;
        sseHub.sendEvent(userId, 'evidence_upload_failed', {
          userId,
          error: errorResponse.error,
          message: errorResponse.message,
          statusCode: statusCode,
          errorDetails: errorResponse.details || null,
          timestamp: new Date().toISOString()
        });
      } catch (sseError) {
        logger.debug('Failed to send SSE event for upload error', { error: sseError });
      }

      return res.status(statusCode).json(errorResponse);
    }
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in upload endpoint', {
      error: error?.message || String(error),
      stack: error?.stack
    });

    res.status(500).json({
      success: false,
      error: 'Failed to process upload request',
      message: error?.message || String(error)
    });
  }
});

/**
 * GET /api/evidence/sources
 * List all connected evidence sources
 */
router.get('/sources', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    // Try user_id first, fallback to seller_id if needed
    let { data: sources, error } = await supabase
      .from('evidence_sources')
      .select('id, provider, account_email, status, last_sync_at, created_at, metadata')
      .eq('user_id', userId)
      .order('created_at', { ascending: false });

    // If user_id doesn't exist, try seller_id
    if (error && error.message?.includes('column') && error.message?.includes('user_id')) {
      const retry = await supabase
        .from('evidence_sources')
        .select('id, provider, account_email, status, last_sync_at, created_at, metadata')
        .eq('seller_id', userId)
        .order('created_at', { ascending: false });
      sources = retry.data;
      error = retry.error;
    }

    if (error) {
      logger.error('âŒ [EVIDENCE] Error fetching evidence sources', {
        error: error.message,
        userId
      });
      return res.status(500).json({
        success: false,
        error: 'Failed to fetch evidence sources',
        message: error.message
      });
    }

    res.json({
      success: true,
      sources: sources || [],
      count: sources?.length || 0
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in sources endpoint', {
      error: error?.message || String(error)
    });

    res.status(500).json({
      success: false,
      error: 'Failed to get evidence sources',
      message: error?.message || String(error)
    });
  }
});

/**
 * GET /api/evidence/sources/:id
 * Get specific evidence source details
 */
router.get('/sources/:id', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;
    const { id } = req.params;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    // Try user_id first, fallback to seller_id if needed
    let { data: source, error } = await supabase
      .from('evidence_sources')
      .select('*')
      .eq('id', id)
      .eq('user_id', userId)
      .single();

    // If user_id doesn't exist, try seller_id
    if (error && error.message?.includes('column') && error.message?.includes('user_id')) {
      const retry = await supabase
        .from('evidence_sources')
        .select('*')
        .eq('id', id)
        .eq('seller_id', userId)
        .single();
      source = retry.data;
      error = retry.error;
    }

    if (error) {
      if (error.code === 'PGRST116') {
        return res.status(404).json({
          success: false,
          error: 'Evidence source not found'
        });
      }
      logger.error('âŒ [EVIDENCE] Error fetching evidence source', {
        error: error.message,
        sourceId: id,
        userId
      });
      return res.status(500).json({
        success: false,
        error: 'Failed to fetch evidence source',
        message: error.message
      });
    }

    res.json({
      success: true,
      source: source
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in source details endpoint', {
      error: error?.message || String(error)
    });

    res.status(500).json({
      success: false,
      error: 'Failed to get evidence source',
      message: error?.message || String(error)
    });
  }
});

/**
 * GET /api/evidence/sources/:id/status
 * Check connection status of evidence source
 */
router.get('/sources/:id/status', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;
    const { id } = req.params;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    // Try user_id first, fallback to seller_id if needed
    let { data: source, error } = await supabase
      .from('evidence_sources')
      .select('id, provider, status, last_sync_at, metadata')
      .eq('id', id)
      .eq('user_id', userId)
      .single();

    // If user_id doesn't exist, try seller_id
    if (error && error.message?.includes('column') && error.message?.includes('user_id')) {
      const retry = await supabase
        .from('evidence_sources')
        .select('id, provider, status, last_sync_at, metadata')
        .eq('id', id)
        .eq('seller_id', userId)
        .single();
      source = retry.data;
      error = retry.error;
    }

    if (error) {
      if (error.code === 'PGRST116') {
        return res.status(404).json({
          success: false,
          error: 'Evidence source not found'
        });
      }
      return res.status(500).json({
        success: false,
        error: 'Failed to fetch source status',
        message: error.message
      });
    }

    // Check if token is still valid (basic check)
    const hasToken = !!(source.metadata?.access_token);
    const isConnected = source.status === 'connected' && hasToken;

    res.json({
      success: true,
      status: {
        connected: isConnected,
        status: source.status,
        lastSync: source.last_sync_at,
        hasToken: hasToken,
        provider: source.provider
      }
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error checking source status', {
      error: error?.message || String(error)
    });

    res.status(500).json({
      success: false,
      error: 'Failed to check source status',
      message: error?.message || String(error)
    });
  }
});

/**
 * DELETE /api/evidence/sources/:id
 * Disconnect evidence source
 */
router.delete('/sources/:id', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;
    const { id } = req.params;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    // Update status to disconnected instead of deleting (preserve history)
    // Try user_id first, fallback to seller_id if needed
    let { data: source, error } = await supabase
      .from('evidence_sources')
      .update({
        status: 'disconnected',
        updated_at: new Date().toISOString()
      })
      .eq('id', id)
      .eq('user_id', userId)
      .select('id, provider')
      .single();

    // If user_id doesn't exist, try seller_id
    if (error && error.message?.includes('column') && error.message?.includes('user_id')) {
      const retry = await supabase
        .from('evidence_sources')
        .update({
          status: 'disconnected',
          updated_at: new Date().toISOString()
        })
        .eq('id', id)
        .eq('seller_id', userId)
        .select('id, provider')
        .single();
      source = retry.data;
      error = retry.error;
    }

    if (error) {
      if (error.code === 'PGRST116') {
        return res.status(404).json({
          success: false,
          error: 'Evidence source not found'
        });
      }
      logger.error('âŒ [EVIDENCE] Error disconnecting evidence source', {
        error: error.message,
        sourceId: id,
        userId
      });
      return res.status(500).json({
        success: false,
        error: 'Failed to disconnect evidence source',
        message: error.message
      });
    }

    logger.info('âœ… [EVIDENCE] Evidence source disconnected', {
      sourceId: id,
      provider: source.provider,
      userId
    });

    res.json({
      success: true,
      message: 'Evidence source disconnected successfully',
      source: {
        id: source.id,
        provider: source.provider,
        status: 'disconnected'
      }
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in disconnect endpoint', {
      error: error?.message || String(error)
    });

    res.status(500).json({
      success: false,
      error: 'Failed to disconnect evidence source',
      message: error?.message || String(error)
    });
  }
});

/**
 * GET /api/evidence/matching/results
 * Get evidence matching results for the current user
 */
router.get('/matching/results', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    const limit = parseInt(req.query.limit as string) || 100;
    const offset = parseInt(req.query.offset as string) || 0;

    logger.info('ðŸ” [EVIDENCE] Fetching matching results', {
      userId,
      limit,
      offset
    });

    // Get matching results from dispute_evidence_links
    // Join with detection_results to get claim info and evidence_documents to get document info
    const client = supabaseAdmin || supabase;

    // First, get all detection_results (claims) for this user
    const { data: claims, error: claimsError } = await client
      .from('detection_results')
      .select('id')
      .eq('seller_id', userId);

    if (claimsError) {
      logger.error('âŒ [EVIDENCE] Error fetching claims', {
        error: claimsError.message,
        userId
      });
      return res.status(500).json({
        success: false,
        error: 'Failed to fetch claims',
        message: claimsError.message
      });
    }

    const claimIds = claims?.map(c => c.id) || [];

    if (claimIds.length === 0) {
      return res.json({
        success: true,
        results: [],
        total: 0
      });
    }

    // Get matching results
    const { data: links, error: linksError } = await client
      .from('dispute_evidence_links')
      .select(`
        id,
        dispute_case_id,
        evidence_document_id,
        relevance_score,
        matched_context,
        created_at
      `)
      .in('dispute_case_id', claimIds)
      .order('created_at', { ascending: false })
      .range(offset, offset + limit - 1);

    if (linksError) {
      logger.error('âŒ [EVIDENCE] Error fetching matching results', {
        error: linksError.message,
        userId
      });
      return res.status(500).json({
        success: false,
        error: 'Failed to fetch matching results',
        message: linksError.message
      });
    }

    // Get total count
    const { count, error: countError } = await client
      .from('dispute_evidence_links')
      .select('*', { count: 'exact', head: true })
      .in('dispute_case_id', claimIds);

    // Transform results
    const results = (links || []).map((link: any) => {
      const context = link.matched_context || {};
      return {
        id: link.id,
        claim_id: link.dispute_case_id,
        document_id: link.evidence_document_id,
        confidence_score: link.relevance_score || 0,
        match_type: context.match_type || 'unknown',
        action_taken: (link.relevance_score || 0) >= 0.85 ? 'auto_submit'
          : (link.relevance_score || 0) >= 0.5 ? 'smart_prompt'
            : 'no_action',
        matched_fields: context.matched_fields || [],
        reasoning: context.reasoning || '',
        created_at: link.created_at
      };
    });

    logger.info('âœ… [EVIDENCE] Matching results fetched', {
      userId,
      count: results.length,
      total: count || 0
    });

    res.json({
      success: true,
      results,
      total: count || 0,
      limit,
      offset
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in matching results endpoint', {
      error: error?.message || String(error),
      stack: error?.stack
    });

    res.status(500).json({
      success: false,
      error: 'Failed to fetch matching results',
      message: error?.message || String(error)
    });
  }
});

/**
 * GET /api/evidence/matching/results/by-document/:documentId
 * Get matching results for a specific document
 */
router.get('/matching/results/by-document/:documentId', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;
    const { documentId } = req.params;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    logger.info('ðŸ” [EVIDENCE] Fetching matching results for document', {
      userId,
      documentId
    });

    const client = supabaseAdmin || supabase;

    // Verify document belongs to user
    const { data: doc, error: docError } = await client
      .from('evidence_documents')
      .select('id, seller_id')
      .eq('id', documentId)
      .single();

    if (docError || !doc) {
      return res.status(404).json({
        success: false,
        error: 'Document not found'
      });
    }

    if (doc.seller_id !== userId) {
      return res.status(403).json({
        success: false,
        error: 'Forbidden'
      });
    }

    // Get matching results for this document
    const { data: links, error: linksError } = await client
      .from('dispute_evidence_links')
      .select(`
        id,
        dispute_case_id,
        evidence_document_id,
        relevance_score,
        matched_context,
        created_at
      `)
      .eq('evidence_document_id', documentId)
      .order('created_at', { ascending: false });

    if (linksError) {
      logger.error('âŒ [EVIDENCE] Error fetching document matching results', {
        error: linksError.message,
        userId,
        documentId
      });
      return res.status(500).json({
        success: false,
        error: 'Failed to fetch matching results',
        message: linksError.message
      });
    }

    // Transform results
    const results = (links || []).map((link: any) => {
      const context = link.matched_context || {};
      return {
        id: link.id,
        claim_id: link.dispute_case_id,
        document_id: link.evidence_document_id,
        confidence_score: link.relevance_score || 0,
        match_type: context.match_type || 'unknown',
        action_taken: (link.relevance_score || 0) >= 0.85 ? 'auto_submit'
          : (link.relevance_score || 0) >= 0.5 ? 'smart_prompt'
            : 'no_action',
        matched_fields: context.matched_fields || [],
        reasoning: context.reasoning || '',
        created_at: link.created_at
      };
    });

    logger.info('âœ… [EVIDENCE] Document matching results fetched', {
      userId,
      documentId,
      count: results.length
    });

    res.json({
      success: true,
      results,
      document_id: documentId
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error in document matching results endpoint', {
      error: error?.message || String(error),
      stack: error?.stack
    });

    res.status(500).json({
      success: false,
      error: 'Failed to fetch document matching results',
      message: error?.message || String(error)
    });
  }
});

/**
 * GET /api/v1/evidence/documents/:documentId
 * Get document with parsed data (for DocumentDetail page)
 */
router.get('/v1/evidence/documents/:documentId', async (req: Request, res: Response) => {
  try {
    const userId = (req as any).userId || (req as any).user?.id || (req as any).user?.user_id;
    const documentId = req.params.documentId;

    if (!userId) {
      return res.status(401).json({
        success: false,
        error: 'Unauthorized'
      });
    }

    logger.info('ðŸ“„ [EVIDENCE] Fetching document with parsed data', {
      userId,
      documentId
    });

    // Fetch document from evidence_documents
    const { data: doc, error: docError } = await supabaseAdmin
      .from('evidence_documents')
      .select('*')
      .eq('id', documentId)
      .single();

    if (docError) {
      if (docError.code === 'PGRST116') {
        return res.status(404).json({
          success: false,
          error: 'Document not found'
        });
      }
      throw docError;
    }

    // Extract parsed data from metadata or document fields
    const metadata = doc.metadata || {};
    const parsedMetadata = metadata.parsed_data || metadata.parsed_metadata || metadata;

    // Build response in expected format
    const response = {
      id: doc.id,
      filename: doc.filename || doc.original_filename,
      processing_status: doc.status || 'completed',
      parser_status: metadata.parser_status || doc.parser_status || 'completed',
      parser_confidence: metadata.parser_confidence || metadata.confidence_score || doc.parser_confidence || 0.95,
      parsed_metadata: {
        supplier_name: parsedMetadata.supplier_name || parsedMetadata.supplier || doc.supplier_name,
        invoice_number: parsedMetadata.invoice_number || parsedMetadata.invoice_no || doc.invoice_number,
        invoice_date: parsedMetadata.invoice_date || parsedMetadata.date,
        total_amount: parsedMetadata.total_amount || parsedMetadata.total || parsedMetadata.amount,
        currency: parsedMetadata.currency || 'USD',
        line_items: parsedMetadata.line_items || parsedMetadata.items || [],
        confidence_score: parsedMetadata.confidence_score || metadata.parser_confidence || 0.95
      },
      // Include raw doc data for fallback
      name: doc.filename || doc.original_filename,
      uploadDate: doc.created_at,
      status: doc.status,
      size: doc.size_bytes,
      type: doc.content_type,
      supplier: parsedMetadata.supplier_name || parsedMetadata.supplier,
      invoice: parsedMetadata.invoice_number || parsedMetadata.invoice_no,
      // Include extractedData if available (for legacy compatibility)
      extractedData: parsedMetadata.line_items || parsedMetadata.items || []
    };

    logger.info('âœ… [EVIDENCE] Document with parsed data fetched', {
      documentId,
      hasMetadata: !!metadata,
      hasParsedData: !!parsedMetadata.supplier_name || !!parsedMetadata.invoice_number
    });

    res.json(response);
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error fetching document with parsed data', {
      documentId: req.params.documentId,
      error: error?.message || String(error),
      stack: error?.stack
    });

    res.status(500).json({
      success: false,
      error: 'Failed to fetch document with parsed data',
      message: error?.message || String(error)
    });
  }
});

/**
 * POST /api/v1/evidence/parse/:documentId
 * Trigger document parsing for a specific document
 */
router.post('/v1/evidence/parse/:documentId', async (req: Request, res: Response) => {
  try {
    const documentId = req.params.documentId;
    const userId = (req as any).userId || (req as any).user?.id || 'demo-user';

    logger.info('ðŸ“„ [EVIDENCE] Triggering document parsing', { documentId, userId });

    // Reset parser_status to 'pending' so the worker picks it up
    const { error: updateError } = await supabaseAdmin
      .from('evidence_documents')
      .update({
        parser_status: 'pending',
        parsed_metadata: null,
        updated_at: new Date().toISOString()
      })
      .eq('id', documentId);

    if (updateError) {
      throw updateError;
    }

    // Try to trigger immediate parsing via the parsing service
    let jobId = `job-${documentId}-${Date.now()}`;
    let message = 'Document queued for parsing. It will be processed by the parsing worker within 2 minutes.';

    try {
      const documentParsingWorker = (await import('../workers/documentParsingWorker')).default;

      // Get document details
      const doc = await documentParsingWorker.getDocumentById(documentId);
      if (doc) {
        // Trigger immediate parsing
        const result = await documentParsingWorker.triggerManualParsing(documentId, doc.seller_id);
        if (result.success) {
          message = 'Document parsing triggered successfully. Results will be available shortly.';
        }
      }
    } catch (workerError: any) {
      logger.debug('Could not trigger immediate parsing, document will be picked up by worker', {
        error: workerError.message
      });
    }

    res.json({
      job_id: jobId,
      status: 'pending',
      message: message,
      estimated_completion: new Date(Date.now() + 120000).toISOString() // 2 minutes from now
    });
  } catch (error: any) {
    logger.error('âŒ [EVIDENCE] Error triggering document parsing', {
      documentId: req.params.documentId,
      error: error?.message || String(error)
    });

    res.status(500).json({
      success: false,
      error: 'Failed to trigger document parsing',
      message: error?.message || String(error)
    });
  }
});

export default router;


