/**
 * Agent 2: Continuous Data Sync / Classification Agent
 * 
 * Purpose:
 * - Receives raw FBA data from Agent 1 (OAuth credentials)
 * - Normalizes, validates, and enriches the data
 * - Calls Discovery Agent (Python ML) for claim detection
 * - Handles mock data generation for sandbox mode
 * - Logs events to agent_events table
 * 
 * Key Responsibilities:
 * 1. Continuous Data Sync - periodic background jobs
 * 2. Data Normalization - converts Amazon raw JSON to internal schema
 * 3. Event Logging - logs to agent_events table
 * 4. Error Handling & Retries - handles API failures gracefully
 * 5. Discovery Agent Integration - calls Python ML API for predictions
 */

import * as fs from 'fs';
import * as path from 'path';
import logger from '../utils/logger';
import tokenManager from '../utils/tokenManager';
import { supabaseAdmin, supabase } from '../database/supabaseClient';
import amazonService from './amazonService';
import { OrdersService } from './ordersService';
import { ShipmentsService } from './shipmentsService';
import { ReturnsService } from './returnsService';
import { SettlementsService } from './settlementsService';
import { MockDataGenerator, MockScenario } from './mockDataGenerator';
import agentEventLogger from './agentEventLogger';
import { upsertDisputesAndRecoveriesFromDetections } from './disputeBackfillService';
import axios from 'axios';
import sseHub from '../utils/sseHub';
import { withErrorHandling } from '../utils/errorHandlingUtils';
import { validateClaim } from '../utils/claimValidation';
import { preventDuplicateClaim } from '../utils/duplicateDetection';

export interface SyncResult {
  success: boolean;
  syncId: string;
  userId: string;
  summary: {
    ordersCount: number;
    shipmentsCount: number;
    returnsCount: number;
    settlementsCount: number;
    inventoryCount: number;
    claimsCount: number;
    feesCount: number;
    claimsDetected?: number;
    detectionId?: string;
  };
  normalized: {
    orders: any[];
    shipments: any[];
    returns: any[];
    settlements: any[];
    inventory: any[];
    claims: any[];
  };
  errors: string[];
  duration: number; // ms
  isMock: boolean;
  mockScenario?: MockScenario;
  // Detection results from Agent 3 (Discovery Agent)
  detectionResult?: {
    totalDetected: number;
    detectionId?: string;
    completed: boolean;
    error?: string;
    skipped?: boolean;
    reason?: string;
  };
}

export class Agent2DataSyncService {
  private ordersService: OrdersService;
  private shipmentsService: ShipmentsService;
  private returnsService: ReturnsService;
  private settlementsService: SettlementsService;
  private readonly pythonApiUrl = process.env.PYTHON_API_URL || 'https://python-api-10.onrender.com';
  private readonly buildVersion = 'v2025.11.25.fix-agent3'; // Build marker for deployment verification
  private readonly BATCH_SIZE = 1000; // Process 1000 records per batch for large datasets

  constructor() {
    this.ordersService = new OrdersService();
    this.shipmentsService = new ShipmentsService();
    this.returnsService = new ReturnsService();
    this.settlementsService = new SettlementsService();
  }

  /**
   * Send sync log event via SSE for real-time frontend updates
   */
  private sendSyncLog(
    userId: string,
    syncId: string,
    log: {
      type: 'info' | 'success' | 'warning' | 'error' | 'progress' | 'thinking';
      category: 'orders' | 'inventory' | 'shipments' | 'returns' | 'settlements' | 'fees' | 'claims' | 'detection' | 'system';
      message: string;
      count?: number;
      context?: {
        details?: string[];
        estimatedTime?: string;
      };
    }
  ): void {
    sseHub.sendEvent(userId, 'sync.log', {
      type: 'log',
      syncId,
      log: {
        ...log,
        timestamp: new Date().toISOString()
      }
    });
    // Also send as 'message' for backward compatibility
    sseHub.sendEvent(userId, 'message', {
      type: 'log',
      syncId,
      log: {
        ...log,
        timestamp: new Date().toISOString()
      }
    });
  }

  /**
   * Main sync method - orchestrates all data fetching and normalization
   */
  async syncUserData(
    userId: string,
    startDate?: Date,
    endDate?: Date,
    parentSyncId?: string
  ): Promise<SyncResult> {
    const syncId = `agent2_sync_${userId}_${Date.now()}`;
    const detectionSyncId = parentSyncId || syncId;
    const startTime = Date.now();
    const errors: string[] = [];

    // Build version marker for deployment verification
    console.log(`[AGENT 2] Build: ${this.buildVersion} - Starting sync for ${userId}`);

    logger.info('üîÑ [AGENT 2] Starting data sync', {
      userId,
      syncId,
      buildVersion: this.buildVersion,
      startDate: startDate?.toISOString(),
      endDate: endDate?.toISOString()
    });

    // Check if user has valid Amazon token from Agent 1
    const isConnected = await tokenManager.isTokenValid(userId, 'amazon');
    const isMockMode = !isConnected || process.env.ENABLE_MOCK_SP_API === 'true' || process.env.USE_MOCK_DATA_GENERATOR !== 'false';
    const mockScenario: MockScenario = (process.env.MOCK_SCENARIO as MockScenario) || 'normal_week';

    if (isMockMode) {
      logger.info('üß™ [AGENT 2] Using mock data generator (sandbox mode)', {
        userId,
        syncId,
        scenario: mockScenario
      });
    }

    // Default to last 18 months for initial sync
    const syncStartDate = startDate || new Date(Date.now() - 18 * 30 * 24 * 60 * 60 * 1000);
    const syncEndDate = endDate || new Date();

    const result: SyncResult = {
      success: true,
      syncId,
      userId,
      summary: {
        ordersCount: 0,
        shipmentsCount: 0,
        returnsCount: 0,
        settlementsCount: 0,
        inventoryCount: 0,
        claimsCount: 0,
        feesCount: 0
      },
      normalized: {
        orders: [],
        shipments: [],
        returns: [],
        settlements: [],
        inventory: [],
        claims: []
      },
      errors: [],
      duration: 0,
      isMock: isMockMode,
      mockScenario: isMockMode ? mockScenario : undefined
    };

    try {
      // 1. Sync Orders
      try {
        this.sendSyncLog(userId, syncId, {
          type: 'thinking',
          category: 'orders',
          message: 'Analyzing seller\'s order history...',
          context: {
            details: [
              `Scanning transaction ledger from ${syncStartDate.toLocaleDateString()} to ${syncEndDate.toLocaleDateString()}`,
              'Cross-referencing with Amazon API data',
              'Checking for fee discrepancies'
            ]
          }
        });
        logger.info('üì¶ [AGENT 2] Fetching orders...', { userId, syncId });

        const ordersResult = await this.syncOrders(userId, syncStartDate, syncEndDate, isMockMode, mockScenario, syncId);
        result.normalized.orders = ordersResult.data || [];
        result.summary.ordersCount = result.normalized.orders.length;

        if (result.summary.ordersCount > 0) {
          const totalVolume = result.normalized.orders.reduce((sum: number, o: any) => sum + (Number(o.total_amount) || 0), 0);
          const avgValue = totalVolume / result.summary.ordersCount;

          this.sendSyncLog(userId, syncId, {
            type: 'success',
            category: 'orders',
            message: `[FOUND] ${result.summary.ordersCount.toLocaleString()} orders in ledger ($${totalVolume.toLocaleString('en-US', { minimumFractionDigits: 2, maximumFractionDigits: 2 })} volume)`,
            count: result.summary.ordersCount,
            context: {
              details: [
                `Average order value: $${avgValue.toLocaleString('en-US', { minimumFractionDigits: 2, maximumFractionDigits: 2 })}`,
                `Date range: ${syncStartDate.toLocaleDateString()} - ${syncEndDate.toLocaleDateString()}`
              ]
            }
          });
          this.sendSyncLog(userId, syncId, {
            type: 'thinking',
            category: 'orders',
            message: 'Normalizing order data structure...',
            context: {
              details: [
                'Converting Amazon format to internal schema',
                `Validating ${result.summary.ordersCount} order records`
              ]
            }
          });
        } else {
          this.sendSyncLog(userId, syncId, {
            type: 'info',
            category: 'orders',
            message: 'No orders found in the specified date range'
          });
        }

        logger.info('‚úÖ [AGENT 2] Orders synced', {
          userId,
          syncId,
          count: result.summary.ordersCount
        });
      } catch (error: any) {
        const errorMsg = `Failed to sync orders: ${error.message}`;
        errors.push(errorMsg);
        this.sendSyncLog(userId, syncId, {
          type: 'error',
          category: 'orders',
          message: `[ERROR] Failed to sync orders: ${error.message}`
        });
        logger.error('‚ùå [AGENT 2] Orders sync failed', { userId, syncId, error: error.message });
      }

      // 2. Sync Shipments
      try {
        this.sendSyncLog(userId, syncId, {
          type: 'thinking',
          category: 'shipments',
          message: 'Checking for shipping discrepancies...',
          context: {
            details: [
              'Querying fulfillment center records',
              `Expected: ~${Math.round(result.summary.ordersCount * 0.85)} shipments based on order volume`
            ]
          }
        });
        logger.info('üöö [AGENT 2] Fetching shipments...', { userId, syncId });

        const shipmentsResult = await this.syncShipments(userId, syncStartDate, syncEndDate, isMockMode, mockScenario, syncId);
        result.normalized.shipments = shipmentsResult.data || [];
        result.summary.shipmentsCount = result.normalized.shipments.length;

        if (result.summary.shipmentsCount > 0) {
          this.sendSyncLog(userId, syncId, {
            type: 'success',
            category: 'shipments',
            message: `[FOUND] ${result.summary.shipmentsCount.toLocaleString()} shipments to fulfillment centers`,
            count: result.summary.shipmentsCount
          });
          this.sendSyncLog(userId, syncId, {
            type: 'thinking',
            category: 'shipments',
            message: 'Verifying received quantities match shipped quantities...',
            context: {
              details: [
                'Cross-referencing inbound shipment plans',
                'Checking for lost or damaged units'
              ]
            }
          });
        } else {
          this.sendSyncLog(userId, syncId, {
            type: 'info',
            category: 'shipments',
            message: 'No shipments found in the specified date range'
          });
        }

        logger.info('‚úÖ [AGENT 2] Shipments synced', {
          userId,
          syncId,
          count: result.summary.shipmentsCount
        });
      } catch (error: any) {
        const errorMsg = `Failed to sync shipments: ${error.message}`;
        errors.push(errorMsg);
        this.sendSyncLog(userId, syncId, {
          type: 'error',
          category: 'shipments',
          message: `[ERROR] Failed to sync shipments: ${error.message}`
        });
        logger.error('‚ùå [AGENT 2] Shipments sync failed', { userId, syncId, error: error.message });
      }

      // 3. Sync Returns
      try {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'returns',
          message: 'Assessing customer return data...'
        });
        logger.info('‚Ü©Ô∏è [AGENT 2] Fetching returns...', { userId, syncId });

        const returnsResult = await this.syncReturns(userId, syncStartDate, syncEndDate, isMockMode, mockScenario, syncId);
        result.normalized.returns = returnsResult.data || [];
        result.summary.returnsCount = result.normalized.returns.length;

        if (result.summary.returnsCount > 0) {
          this.sendSyncLog(userId, syncId, {
            type: 'success',
            category: 'returns',
            message: `[FOUND] ${result.summary.returnsCount.toLocaleString()} customer returns processed`,
            count: result.summary.returnsCount
          });
          this.sendSyncLog(userId, syncId, {
            type: 'info',
            category: 'returns',
            message: 'Checking if returns were properly credited to seller account...'
          });
        } else {
          this.sendSyncLog(userId, syncId, {
            type: 'info',
            category: 'returns',
            message: 'No returns found in the specified date range'
          });
        }

        logger.info('‚úÖ [AGENT 2] Returns synced', {
          userId,
          syncId,
          count: result.summary.returnsCount
        });
      } catch (error: any) {
        const errorMsg = `Failed to sync returns: ${error.message}`;
        errors.push(errorMsg);
        this.sendSyncLog(userId, syncId, {
          type: 'error',
          category: 'returns',
          message: `[ERROR] Failed to sync returns: ${error.message}`
        });
        logger.error('‚ùå [AGENT 2] Returns sync failed', { userId, syncId, error: error.message });
      }

      // 4. Sync Settlements
      try {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'settlements',
          message: 'Assessing settlement periods and payout data...'
        });
        logger.info('üí∞ [AGENT 2] Fetching settlements...', { userId, syncId });

        const settlementsResult = await this.syncSettlements(userId, syncStartDate, syncEndDate, isMockMode, mockScenario, syncId);
        result.normalized.settlements = settlementsResult.data || [];
        result.summary.settlementsCount = result.normalized.settlements.length;

        if (result.summary.settlementsCount > 0) {
          this.sendSyncLog(userId, syncId, {
            type: 'success',
            category: 'settlements',
            message: `[FOUND] ${result.summary.settlementsCount.toLocaleString()} settlement periods`,
            count: result.summary.settlementsCount
          });
          this.sendSyncLog(userId, syncId, {
            type: 'info',
            category: 'settlements',
            message: 'Reconciling payouts with expected amounts...'
          });
        } else {
          this.sendSyncLog(userId, syncId, {
            type: 'info',
            category: 'settlements',
            message: 'No settlements found in the specified date range'
          });
        }

        logger.info('‚úÖ [AGENT 2] Settlements synced', {
          userId,
          syncId,
          count: result.summary.settlementsCount
        });
      } catch (error: any) {
        const errorMsg = `Failed to sync settlements: ${error.message}`;
        errors.push(errorMsg);
        this.sendSyncLog(userId, syncId, {
          type: 'error',
          category: 'settlements',
          message: `[ERROR] Failed to sync settlements: ${error.message}`
        });
        logger.error('‚ùå [AGENT 2] Settlements sync failed', { userId, syncId, error: error.message });
      }

      // 5. Sync Inventory
      try {
        this.sendSyncLog(userId, syncId, {
          type: 'thinking',
          category: 'inventory',
          message: 'Drilling into inventory database...',
          context: {
            details: [
              'Pulling SKU data from warehouse management',
              'Cross-checking active vs discontinued items'
            ]
          }
        });
        logger.info('üìä [AGENT 2] Fetching inventory...', { userId, syncId });

        const inventoryResult = await this.syncInventory(userId, isMockMode, mockScenario, syncId);
        result.normalized.inventory = inventoryResult.data || [];
        result.summary.inventoryCount = result.normalized.inventory.length;

        if (result.summary.inventoryCount > 0) {
          // Calculate inventory value if price exists, otherwise estimate
          const totalValue = result.normalized.inventory.reduce((sum: number, i: any) => sum + ((Number(i.price) || 25) * (Number(i.quantity) || 0)), 0);
          const avgValue = totalValue / (result.normalized.inventory.reduce((sum: number, i: any) => sum + (Number(i.quantity) || 0), 1) || 1);

          this.sendSyncLog(userId, syncId, {
            type: 'success',
            category: 'inventory',
            message: `[FOUND] ${result.summary.inventoryCount.toLocaleString()} active SKUs in warehouse`,
            count: result.summary.inventoryCount,
            context: {
              details: [
                `Total inventory value: ~$${totalValue.toLocaleString('en-US', { minimumFractionDigits: 2, maximumFractionDigits: 2 })}`,
                `Average unit value: ~$${avgValue.toLocaleString('en-US', { minimumFractionDigits: 2, maximumFractionDigits: 2 })}`
              ]
            }
          });
          this.sendSyncLog(userId, syncId, {
            type: 'thinking',
            category: 'inventory',
            message: 'Checking unit counts against inbound shipments...',
            context: {
              details: [
                'Verifying stock levels',
                'Identifying potential missing units'
              ]
            }
          });
        } else {
          this.sendSyncLog(userId, syncId, {
            type: 'info',
            category: 'inventory',
            message: 'No inventory items found'
          });
        }

        logger.info('‚úÖ [AGENT 2] Inventory synced', {
          userId,
          syncId,
          count: result.summary.inventoryCount
        });
      } catch (error: any) {
        const errorMsg = `Failed to sync inventory: ${error.message}`;
        errors.push(errorMsg);
        this.sendSyncLog(userId, syncId, {
          type: 'error',
          category: 'inventory',
          message: `[ERROR] Failed to sync inventory: ${error.message}`
        });
        logger.error('‚ùå [AGENT 2] Inventory sync failed', { userId, syncId, error: error.message });
      }

      // 6. Sync Claims (from financial events)
      try {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'claims',
          message: 'Assessing financial events for claim opportunities...'
        });
        logger.info('üéØ [AGENT 2] Fetching claims...', { userId, syncId });

        const claimsResult = await this.syncClaims(userId, syncStartDate, syncEndDate, isMockMode, mockScenario, syncId);
        result.normalized.claims = claimsResult.data || [];
        result.summary.claimsCount = result.normalized.claims.length;

        if (result.summary.claimsCount > 0) {
          this.sendSyncLog(userId, syncId, {
            type: 'success',
            category: 'claims',
            message: `[FOUND] ${result.summary.claimsCount.toLocaleString()} potential claim opportunities`,
            count: result.summary.claimsCount
          });
          this.sendSyncLog(userId, syncId, {
            type: 'info',
            category: 'claims',
            message: 'Analyzing claim data for refund eligibility...'
          });
        } else {
          this.sendSyncLog(userId, syncId, {
            type: 'info',
            category: 'claims',
            message: 'No claim opportunities found in financial events'
          });
        }

        logger.info('‚úÖ [AGENT 2] Claims synced', {
          userId,
          syncId,
          count: result.summary.claimsCount
        });
      } catch (error: any) {
        const errorMsg = `Failed to sync claims: ${error.message}`;
        errors.push(errorMsg);
        this.sendSyncLog(userId, syncId, {
          type: 'error',
          category: 'claims',
          message: `[ERROR] Failed to sync claims: ${error.message}`
        });
        logger.error('‚ùå [AGENT 2] Claims sync failed', { userId, syncId, error: error.message });
      }

      // Calculate duration
      result.duration = Date.now() - startTime;
      result.errors = errors;
      result.success = errors.length === 0;

      // Log event to agent_events table (using generic logEvent method if available)
      try {
        // Use a simple direct insert since agent_events might not have 'data_sync' as an agent type yet
        const { error: logError } = await supabaseAdmin
          .from('agent_events')
          .insert({
            user_id: userId,
            agent: 'data_sync', // Will need migration update, but works for now
            event_type: result.success ? 'sync_completed' : 'sync_failed',
            success: result.success,
            metadata: {
              syncId,
              summary: result.summary,
              duration: result.duration,
              isMock: result.isMock,
              mockScenario: result.mockScenario,
              errors: result.errors
            },
            created_at: new Date().toISOString()
          });

        if (logError) {
          logger.warn('‚ö†Ô∏è [AGENT 2] Failed to log event (may need migration update)', { error: logError.message });
        }
      } catch (logError: any) {
        logger.warn('‚ö†Ô∏è [AGENT 2] Failed to log event', { error: logError.message });
      }

      logger.info('‚úÖ [AGENT 2] Data sync completed', {
        userId,
        syncId,
        success: result.success,
        summary: result.summary,
        duration: result.duration,
        errorsCount: errors.length
      });

      // Step 7: Call Discovery Agent (Python ML) - NOW BLOCKING to show results immediately
      // Changed from async to blocking so detection results appear in sync status
      if (result.success && result.summary.ordersCount + result.summary.shipmentsCount + result.summary.returnsCount > 0) {
        try {
          logger.info('üîç [AGENT 2] Starting Discovery Agent (Python ML) - BLOCKING', { userId, syncId });

          const detectionId = `detection_${userId}_${Date.now()}`;
          const detectionResult = await this.callDiscoveryAgent(
            userId,
            syncId,
            detectionId,
            result.normalized,
            detectionSyncId,
            isMockMode
          );

          // Add detection results to the sync result
          result.detectionResult = {
            totalDetected: detectionResult?.totalDetected || 0,
            detectionId,
            completed: true
          };

          // CRITICAL: Update summary for frontend visibility
          result.summary.claimsDetected = detectionResult?.totalDetected || 0;
          result.summary.detectionId = detectionId;


          logger.info('‚úÖ [AGENT 2] Discovery Agent completed', {
            userId,
            syncId,
            totalDetected: detectionResult?.totalDetected
          });
        } catch (detectionError: any) {
          logger.error('‚ùå [AGENT 2] Discovery Agent failed', {
            error: detectionError.message,
            stack: detectionError.stack,
            userId,
            syncId
          });
          // Still mark detection as attempted but failed
          result.detectionResult = {
            totalDetected: 0,
            error: detectionError.message,
            completed: false
          };
          // Don't fail the overall sync - detection errors are logged but sync continues
        }
      } else {
        // No data to detect
        result.detectionResult = {
          totalDetected: 0,
          completed: true,
          skipped: true,
          reason: 'No data to analyze'
        };
      }

      return result;
    } catch (error: any) {
      result.success = false;
      result.errors.push(`Fatal error: ${error.message}`);
      result.duration = Date.now() - startTime;

      logger.error('‚ùå [AGENT 2] Fatal sync error', {
        userId,
        syncId,
        error: error.message,
        stack: error.stack
      });

      // Log error event
      try {
        const { error: logError } = await supabaseAdmin
          .from('agent_events')
          .insert({
            user_id: userId,
            agent: 'data_sync',
            event_type: 'sync_failed',
            success: false,
            metadata: {
              syncId,
              error: error.message,
              duration: result.duration
            },
            created_at: new Date().toISOString()
          });

        if (logError) {
          logger.warn('‚ö†Ô∏è [AGENT 2] Failed to log error event', { error: logError.message });
        }
      } catch (logError: any) {
        logger.warn('‚ö†Ô∏è [AGENT 2] Failed to log error event', { error: logError.message });
      }

      return result;
    }
  }

  /**
   * Sync Orders - ONLY 2 DATA SOURCES:
   * 1. Real SP-API (production mode when Amazon token is valid)
   * 2. Mock Data Generator (sandbox mode when no token)
   */
  private async syncOrders(
    userId: string,
    startDate: Date,
    endDate: Date,
    isMockMode: boolean,
    mockScenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    // SIMPLE LOGIC: Mock mode = generate, Real mode = SP-API
    if (isMockMode) {
      logger.info('üì¶ [AGENT 2] Using MOCK DATA GENERATOR for orders', { userId, syncId });
      return this.generateMockOrders(userId, startDate, endDate, mockScenario, syncId);
    }

    // Production: Use real SP-API
    logger.info('üì¶ [AGENT 2] Using REAL SP-API for orders', { userId, syncId });
    return await this.ordersService.fetchOrders(userId, startDate, endDate);
  }

  /**
   * Sync Shipments - ONLY 2 DATA SOURCES:
   * 1. Real SP-API (production mode when Amazon token is valid)
   * 2. Mock Data Generator (sandbox mode when no token)
   */
  private async syncShipments(
    userId: string,
    startDate: Date,
    endDate: Date,
    isMockMode: boolean,
    mockScenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    // SIMPLE LOGIC: Mock mode = generate, Real mode = SP-API
    if (isMockMode) {
      logger.info('üöö [AGENT 2] Using MOCK DATA GENERATOR for shipments', { userId, syncId });
      return this.generateMockShipments(userId, startDate, endDate, mockScenario, syncId);
    }

    // Production: Use real SP-API
    logger.info('üöö [AGENT 2] Using REAL SP-API for shipments', { userId, syncId });
    return await this.shipmentsService.fetchShipments(userId, startDate, endDate);
  }

  /**
   * Sync Returns - ONLY 2 DATA SOURCES:
   * 1. Real SP-API (production mode when Amazon token is valid)
   * 2. Mock Data Generator (sandbox mode when no token)
   */
  private async syncReturns(
    userId: string,
    startDate: Date,
    endDate: Date,
    isMockMode: boolean,
    mockScenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    // SIMPLE LOGIC: Mock mode = generate, Real mode = SP-API
    if (isMockMode) {
      logger.info('‚Ü©Ô∏è [AGENT 2] Using MOCK DATA GENERATOR for returns', { userId, syncId });
      return this.generateMockReturns(userId, startDate, endDate, mockScenario, syncId);
    }

    // Production: Use real SP-API
    logger.info('‚Ü©Ô∏è [AGENT 2] Using REAL SP-API for returns', { userId, syncId });
    return await this.returnsService.fetchReturns(userId, startDate, endDate);
  }

  /**
   * Sync Settlements - ONLY 2 DATA SOURCES:
   * 1. Real SP-API (production mode when Amazon token is valid)
   * 2. Mock Data Generator (sandbox mode when no token)
   */
  private async syncSettlements(
    userId: string,
    startDate: Date,
    endDate: Date,
    isMockMode: boolean,
    mockScenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    // SIMPLE LOGIC: Mock mode = generate, Real mode = SP-API
    if (isMockMode) {
      logger.info('üí∞ [AGENT 2] Using MOCK DATA GENERATOR for settlements', { userId, syncId });
      return this.generateMockSettlements(userId, startDate, endDate, mockScenario, syncId);
    }

    // Production: Use real SP-API
    logger.info('üí∞ [AGENT 2] Using REAL SP-API for settlements', { userId, syncId });
    return await this.settlementsService.fetchSettlements(userId, startDate, endDate);
  }

  /**
   * Sync Inventory - ONLY 2 DATA SOURCES:
   * 1. Real SP-API (production mode when Amazon token is valid)
   * 2. Mock Data Generator (sandbox mode when no token)
   */
  private async syncInventory(
    userId: string,
    isMockMode: boolean,
    mockScenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    // SIMPLE LOGIC: Mock mode = generate, Real mode = SP-API
    if (isMockMode) {
      logger.info('üì¶ [AGENT 2] Using MOCK DATA GENERATOR for inventory', { userId, syncId });
      return this.generateMockInventory(userId, mockScenario, syncId);
    }

    // Production: Use real SP-API
    logger.info('üì¶ [AGENT 2] Using REAL SP-API for inventory', { userId, syncId });
    const inventoryResult = await amazonService.fetchInventory(userId);
    const inventory = inventoryResult.data || inventoryResult;
    return {
      success: true,
      data: Array.isArray(inventory) ? inventory : [],
      message: `Fetched ${Array.isArray(inventory) ? inventory.length : 0} inventory items`
    };
  }

  /**
   * Sync Claims with mock fallback and batch processing
   */
  private async syncClaims(
    userId: string,
    startDate: Date,
    endDate: Date,
    isMockMode: boolean,
    mockScenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    if (isMockMode) {
      return this.generateMockClaims(userId, startDate, endDate, mockScenario, syncId);
    }

    const claimsResult = await amazonService.fetchClaims(userId, startDate, endDate);
    const claims = claimsResult.data || claimsResult;
    return {
      success: true,
      data: Array.isArray(claims) ? claims : [],
      message: `Fetched ${Array.isArray(claims) ? claims.length : 0} claims`
    };
  }

  /**
   * Generate mock orders with batch processing for large datasets
   */
  private async generateMockOrders(
    userId: string,
    startDate: Date,
    endDate: Date,
    scenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    // Use 75 as default to match amazonService.ts and backend logs
    const recordCount = process.env.MOCK_RECORD_COUNT ? parseInt(process.env.MOCK_RECORD_COUNT, 10) : 75;
    const generator = new MockDataGenerator({
      scenario,
      recordCount,
      startDate,
      endDate
    });

    const allOrders: any[] = [];
    const daysDiff = Math.ceil((endDate.getTime() - startDate.getTime()) / (1000 * 60 * 60 * 24));

    // Batch processing for large datasets (1000+ records)
    const needsBatching = recordCount > this.BATCH_SIZE;
    const totalBatches = needsBatching ? Math.ceil(recordCount / this.BATCH_SIZE) : 1;

    if (syncId) {
      if (needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'orders',
          message: `Processing ${recordCount.toLocaleString()} orders in ${totalBatches} batches...`
        });
      } else {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'orders',
          message: `Processing ${recordCount.toLocaleString()} orders...`
        });
      }
    }

    for (let batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
      const batchStart = batchIndex * this.BATCH_SIZE;
      const batchEnd = Math.min(batchStart + this.BATCH_SIZE, recordCount);
      const batchSize = batchEnd - batchStart;

      if (syncId && needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'progress',
          category: 'orders',
          message: `Processing batch ${batchIndex + 1}/${totalBatches} (${batchStart + 1}-${batchEnd} of ${recordCount.toLocaleString()})...`
        });
      }

      const batchOrders: any[] = [];
      for (let i = batchStart; i < batchEnd; i++) {
        const orderDate = this.randomDate(startDate, endDate);
        const orderId = `112-${Math.floor(Math.random() * 10000000)}-${Math.floor(Math.random() * 1000000)}`;

        batchOrders.push({
          AmazonOrderId: orderId,
          PurchaseDate: orderDate.toISOString(),
          OrderStatus: ['Shipped', 'Pending', 'Canceled'][Math.floor(Math.random() * 3)],
          FulfillmentChannel: ['Amazon', 'Merchant'][Math.floor(Math.random() * 2)],
          OrderItems: [{
            SellerSKU: `SKU-${String(Math.floor(Math.random() * 1000)).padStart(4, '0')}`,
            ASIN: `B0${String(Math.floor(Math.random() * 10000000)).padStart(8, '0')}`,
            QuantityOrdered: Math.floor(Math.random() * 5) + 1,
            ItemPrice: {
              Amount: (Math.random() * 100 + 10).toFixed(2),
              CurrencyCode: 'USD'
            },
            Title: `Product ${i + 1}`
          }],
          MarketplaceId: 'ATVPDKIKX0DER',
          isMock: true,
          mockScenario: scenario
        });
      }

      allOrders.push(...batchOrders);

      // Small delay between batches to avoid overwhelming the system
      if (needsBatching && batchIndex < totalBatches - 1) {
        await new Promise(resolve => setTimeout(resolve, 10)); // 10ms delay
      }
    }

    // Normalize orders in batches if needed
    let normalized: any[] = [];
    if (allOrders.length > this.BATCH_SIZE) {
      // Normalize in batches to avoid memory issues
      const normalizeBatches = Math.ceil(allOrders.length / this.BATCH_SIZE);
      for (let i = 0; i < normalizeBatches; i++) {
        const batchStart = i * this.BATCH_SIZE;
        const batchEnd = Math.min(batchStart + this.BATCH_SIZE, allOrders.length);
        const batch = allOrders.slice(batchStart, batchEnd);
        const normalizedBatch = this.ordersService.normalizeOrders(batch, userId);
        normalized.push(...normalizedBatch);
      }
    } else {
      normalized = this.ordersService.normalizeOrders(allOrders, userId);
    }

    if (syncId) {
      this.sendSyncLog(userId, syncId, {
        type: 'success',
        category: 'orders',
        message: `[COMPLETE] Generated and normalized ${normalized.length.toLocaleString()} orders`
      });
    }

    return {
      success: true,
      data: normalized,
      message: `Generated ${normalized.length} mock orders (scenario: ${scenario})`
    };
  }

  /**
   * Generate mock shipments with batch processing
   */
  private async generateMockShipments(
    userId: string,
    startDate: Date,
    endDate: Date,
    scenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    // Use 75 as base default to match orders
    const recordCount = Math.floor((process.env.MOCK_RECORD_COUNT ? parseInt(process.env.MOCK_RECORD_COUNT, 10) : 75) * 0.7);
    const allShipments: any[] = [];

    // Determine how many shipments should have issues (missing quantity)
    const issueRate = scenario === 'with_issues' ? 0.4 : scenario === 'high_volume' ? 0.2 : 0.15;

    // Batch processing for large datasets
    const needsBatching = recordCount > this.BATCH_SIZE;
    const totalBatches = needsBatching ? Math.ceil(recordCount / this.BATCH_SIZE) : 1;

    if (syncId) {
      if (needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'shipments',
          message: `Processing ${recordCount.toLocaleString()} shipments in ${totalBatches} batches...`
        });
      } else {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'shipments',
          message: `Processing ${recordCount.toLocaleString()} shipments...`
        });
      }
    }

    for (let batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
      const batchStart = batchIndex * this.BATCH_SIZE;
      const batchEnd = Math.min(batchStart + this.BATCH_SIZE, recordCount);

      if (syncId && needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'progress',
          category: 'shipments',
          message: `Processing batch ${batchIndex + 1}/${totalBatches} (${batchStart + 1}-${batchEnd} of ${recordCount.toLocaleString()})...`
        });
      }

      const batchShipments: any[] = [];
      for (let i = batchStart; i < batchEnd; i++) {
        const shippedDate = this.randomDate(startDate, endDate);
        const receivedDate = new Date(shippedDate.getTime() + Math.random() * 7 * 24 * 60 * 60 * 1000);

        const quantityShipped = Math.floor(Math.random() * 10) + 1;
        const hasIssue = Math.random() < issueRate;
        // For issues, reduce received quantity to create missing_quantity
        const quantityReceived = hasIssue ? Math.max(0, quantityShipped - Math.floor(Math.random() * 3) - 1) : quantityShipped;
        const itemPrice = parseFloat((Math.random() * 50 + 10).toFixed(2)); // $10-60 per item

        batchShipments.push({
          ShipmentId: `SHIP-${Date.now()}-${i}`,
          ShippedDate: shippedDate.toISOString(),
          ReceivedDate: receivedDate.toISOString(),
          Status: hasIssue ? 'CHECKED_IN' : ['RECEIVED', 'IN_TRANSIT', 'CHECKED_IN'][Math.floor(Math.random() * 3)],
          QuantityShipped: quantityShipped,
          QuantityReceived: quantityReceived,
          Items: [{
            SellerSKU: `SKU-${String(Math.floor(Math.random() * 1000)).padStart(4, '0')}`,
            ASIN: `B0${String(Math.floor(Math.random() * 10000000)).padStart(8, '0')}`,
            QuantityShipped: quantityShipped,
            ItemPrice: itemPrice // Add price for value calculation
          }],
          FulfillmentCenterId: `FBA${Math.floor(Math.random() * 5) + 1}`,
          isMock: true,
          mockScenario: scenario
        });
      }

      allShipments.push(...batchShipments);

      // Small delay between batches
      if (needsBatching && batchIndex < totalBatches - 1) {
        await new Promise(resolve => setTimeout(resolve, 10));
      }
    }

    // Normalize in batches if needed
    let normalized: any[] = [];
    if (allShipments.length > this.BATCH_SIZE) {
      const normalizeBatches = Math.ceil(allShipments.length / this.BATCH_SIZE);
      for (let i = 0; i < normalizeBatches; i++) {
        const batchStart = i * this.BATCH_SIZE;
        const batchEnd = Math.min(batchStart + this.BATCH_SIZE, allShipments.length);
        const batch = allShipments.slice(batchStart, batchEnd);
        const normalizedBatch = this.shipmentsService.normalizeShipments(batch, userId);
        normalized.push(...normalizedBatch);
      }
    } else {
      normalized = this.shipmentsService.normalizeShipments(allShipments, userId);
    }

    if (syncId) {
      this.sendSyncLog(userId, syncId, {
        type: 'success',
        category: 'shipments',
        message: `[COMPLETE] Generated and normalized ${normalized.length.toLocaleString()} shipments`
      });
    }

    return {
      success: true,
      data: normalized,
      message: `Generated ${normalized.length} mock shipments (scenario: ${scenario})`
    };
  }

  /**
   * Generate mock returns with batch processing
   */
  private async generateMockReturns(
    userId: string,
    startDate: Date,
    endDate: Date,
    scenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    // Use 75 as base default to match orders
    const recordCount = Math.floor((process.env.MOCK_RECORD_COUNT ? parseInt(process.env.MOCK_RECORD_COUNT, 10) : 75) * 0.5);
    const allReturns: any[] = [];

    // Batch processing for large datasets
    const needsBatching = recordCount > this.BATCH_SIZE;
    const totalBatches = needsBatching ? Math.ceil(recordCount / this.BATCH_SIZE) : 1;

    if (syncId) {
      if (needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'returns',
          message: `Processing ${recordCount.toLocaleString()} returns in ${totalBatches} batches...`
        });
      } else {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'returns',
          message: `Processing ${recordCount.toLocaleString()} returns...`
        });
      }
    }

    for (let batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
      const batchStart = batchIndex * this.BATCH_SIZE;
      const batchEnd = Math.min(batchStart + this.BATCH_SIZE, recordCount);

      if (syncId && needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'progress',
          category: 'returns',
          message: `Processing batch ${batchIndex + 1}/${totalBatches} (${batchStart + 1}-${batchEnd} of ${recordCount.toLocaleString()})...`
        });
      }

      const batchReturns: any[] = [];
      for (let i = batchStart; i < batchEnd; i++) {
        const returnDate = this.randomDate(startDate, endDate);

        batchReturns.push({
          ReturnId: `RET-${Date.now()}-${i}`,
          AmazonOrderId: `112-${Math.floor(Math.random() * 10000000)}-${Math.floor(Math.random() * 1000000)}`,
          ReturnedDate: returnDate.toISOString(),
          ReturnStatus: ['APPROVED', 'PENDING', 'DENIED'][Math.floor(Math.random() * 3)],
          ReturnReason: ['Defective', 'Wrong Item', 'Not as Described', 'Customer Changed Mind'][Math.floor(Math.random() * 4)],
          Items: [{
            SellerSKU: `SKU-${String(Math.floor(Math.random() * 1000)).padStart(4, '0')}`,
            ASIN: `B0${String(Math.floor(Math.random() * 10000000)).padStart(8, '0')}`,
            QuantityReturned: Math.floor(Math.random() * 3) + 1,
            RefundAmount: {
              Amount: (Math.random() * 50 + 10).toFixed(2),
              CurrencyCode: 'USD'
            }
          }],
          isMock: true,
          mockScenario: scenario
        });
      }

      allReturns.push(...batchReturns);

      // Small delay between batches
      if (needsBatching && batchIndex < totalBatches - 1) {
        await new Promise(resolve => setTimeout(resolve, 10));
      }
    }

    // Normalize in batches if needed
    let normalized: any[] = [];
    if (allReturns.length > this.BATCH_SIZE) {
      const normalizeBatches = Math.ceil(allReturns.length / this.BATCH_SIZE);
      for (let i = 0; i < normalizeBatches; i++) {
        const batchStart = i * this.BATCH_SIZE;
        const batchEnd = Math.min(batchStart + this.BATCH_SIZE, allReturns.length);
        const batch = allReturns.slice(batchStart, batchEnd);
        const normalizedBatch = this.returnsService.normalizeReturns(batch, userId);
        normalized.push(...normalizedBatch);
      }
    } else {
      normalized = this.returnsService.normalizeReturns(allReturns, userId);
    }

    if (syncId) {
      this.sendSyncLog(userId, syncId, {
        type: 'success',
        category: 'returns',
        message: `[COMPLETE] Generated and normalized ${normalized.length.toLocaleString()} returns`
      });
    }

    return {
      success: true,
      data: normalized,
      message: `Generated ${normalized.length} mock returns (scenario: ${scenario})`
    };
  }

  /**
   * Generate mock settlements with batch processing
   */
  private async generateMockSettlements(
    userId: string,
    startDate: Date,
    endDate: Date,
    scenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    // Use 75 as base default to match orders
    const recordCount = Math.floor((process.env.MOCK_RECORD_COUNT ? parseInt(process.env.MOCK_RECORD_COUNT, 10) : 75) * 0.6);
    const allSettlements: any[] = [];

    // Determine how many settlements should have fee discrepancies
    const issueRate = scenario === 'with_issues' ? 0.4 : scenario === 'high_volume' ? 0.2 : 0.15;

    // Batch processing for large datasets
    const needsBatching = recordCount > this.BATCH_SIZE;
    const totalBatches = needsBatching ? Math.ceil(recordCount / this.BATCH_SIZE) : 1;

    if (syncId) {
      if (needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'settlements',
          message: `Processing ${recordCount.toLocaleString()} settlements in ${totalBatches} batches...`
        });
      } else {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'settlements',
          message: `Processing ${recordCount.toLocaleString()} settlements...`
        });
      }
    }

    for (let batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
      const batchStart = batchIndex * this.BATCH_SIZE;
      const batchEnd = Math.min(batchStart + this.BATCH_SIZE, recordCount);

      if (syncId && needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'progress',
          category: 'settlements',
          message: `Processing batch ${batchIndex + 1}/${totalBatches} (${batchStart + 1}-${batchEnd} of ${recordCount.toLocaleString()})...`
        });
      }

      const batchSettlements: any[] = [];
      for (let i = batchStart; i < batchEnd; i++) {
        const settlementDate = this.randomDate(startDate, endDate);
        const hasIssue = Math.random() < issueRate;

        const amount = parseFloat((Math.random() * 1000 + 100).toFixed(2));
        // For issues, generate higher fees (potential overcharge)
        const fees = hasIssue
          ? parseFloat((amount * (0.2 + Math.random() * 0.1)).toFixed(2)) // 20-30% fee (high - potential overcharge)
          : parseFloat((amount * 0.15).toFixed(2)); // 15% fee (normal)

        batchSettlements.push({
          SettlementId: `SETTLE-${Date.now()}-${i}`,
          settlement_date: settlementDate.toISOString(),
          amount: amount,
          fees: fees,
          currency: 'USD',
          transaction_type: hasIssue ? 'fee_adjustment' : 'fee',
          fee_breakdown: {
            fba_fulfillment: parseFloat((fees * 0.6).toFixed(2)),
            referral: parseFloat((fees * 0.3).toFixed(2)),
            closing: parseFloat((fees * 0.1).toFixed(2))
          },
          isMock: true,
          mockScenario: scenario
        });
      }

      allSettlements.push(...batchSettlements);

      // Small delay between batches
      if (needsBatching && batchIndex < totalBatches - 1) {
        await new Promise(resolve => setTimeout(resolve, 10));
      }
    }

    // Normalize in batches if needed
    let normalized: any[] = [];
    if (allSettlements.length > this.BATCH_SIZE) {
      const normalizeBatches = Math.ceil(allSettlements.length / this.BATCH_SIZE);
      for (let i = 0; i < normalizeBatches; i++) {
        const batchStart = i * this.BATCH_SIZE;
        const batchEnd = Math.min(batchStart + this.BATCH_SIZE, allSettlements.length);
        const batch = allSettlements.slice(batchStart, batchEnd);
        const normalizedBatch = this.settlementsService.normalizeSettlements(batch, userId);
        normalized.push(...normalizedBatch);
      }
    } else {
      normalized = this.settlementsService.normalizeSettlements(allSettlements, userId);
    }

    if (syncId) {
      this.sendSyncLog(userId, syncId, {
        type: 'success',
        category: 'settlements',
        message: `[COMPLETE] Generated and normalized ${normalized.length.toLocaleString()} settlements`
      });
    }

    return {
      success: true,
      data: normalized,
      message: `Generated ${normalized.length} mock settlements (scenario: ${scenario})`
    };
  }

  /**
   * Generate mock inventory with batch processing
   */
  private async generateMockInventory(
    userId: string,
    scenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    // Use same default as orders (75) for consistency
    const recordCount = process.env.MOCK_RECORD_COUNT ? parseInt(process.env.MOCK_RECORD_COUNT, 10) : 75;
    const allInventory: any[] = [];

    // Batch processing for large datasets
    const needsBatching = recordCount > this.BATCH_SIZE;
    const totalBatches = needsBatching ? Math.ceil(recordCount / this.BATCH_SIZE) : 1;

    if (syncId) {
      if (needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'inventory',
          message: `Processing ${recordCount.toLocaleString()} inventory items in ${totalBatches} batches...`
        });
      } else {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'inventory',
          message: `Processing ${recordCount.toLocaleString()} inventory items...`
        });
      }
    }

    for (let batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
      const batchStart = batchIndex * this.BATCH_SIZE;
      const batchEnd = Math.min(batchStart + this.BATCH_SIZE, recordCount);

      if (syncId && needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'progress',
          category: 'inventory',
          message: `Processing batch ${batchIndex + 1}/${totalBatches} (${batchStart + 1}-${batchEnd} of ${recordCount.toLocaleString()})...`
        });
      }

      const batchInventory: any[] = [];
      for (let i = batchStart; i < batchEnd; i++) {
        batchInventory.push({
          SellerSKU: `SKU-${String(Math.floor(Math.random() * 1000)).padStart(4, '0')}`,
          ASIN: `B0${String(Math.floor(Math.random() * 10000000)).padStart(8, '0')}`,
          FulfillableQuantity: Math.floor(Math.random() * 100) + 1,
          InboundQuantity: Math.floor(Math.random() * 50),
          ReservedQuantity: Math.floor(Math.random() * 10),
          FulfillmentCenterId: `FBA${Math.floor(Math.random() * 5) + 1}`,
          isMock: true,
          mockScenario: scenario
        });
      }

      allInventory.push(...batchInventory);

      // Small delay between batches
      if (needsBatching && batchIndex < totalBatches - 1) {
        await new Promise(resolve => setTimeout(resolve, 10));
      }
    }

    if (syncId) {
      this.sendSyncLog(userId, syncId, {
        type: 'success',
        category: 'inventory',
        message: `[COMPLETE] Generated ${allInventory.length.toLocaleString()} inventory items`
      });
    }

    return {
      success: true,
      data: allInventory,
      message: `Generated ${allInventory.length} mock inventory items (scenario: ${scenario})`
    };
  }

  /**
   * Generate mock claims with batch processing
   */
  private async generateMockClaims(
    userId: string,
    startDate: Date,
    endDate: Date,
    scenario: MockScenario,
    syncId?: string
  ): Promise<{ success: boolean; data: any[]; message: string }> {
    const generator = new MockDataGenerator({
      scenario,
      recordCount: process.env.MOCK_RECORD_COUNT ? parseInt(process.env.MOCK_RECORD_COUNT, 10) : 75,
      startDate,
      endDate
    });

    const financialEvents = generator.generateFinancialEvents();
    const allClaims: any[] = [];

    // Extract reimbursements
    const reimbursements = financialEvents.payload?.FinancialEvents?.FBALiquidationEventList || [];
    const totalRecords = reimbursements.length + (financialEvents.payload?.FinancialEvents?.AdjustmentEventList || []).length;

    // Batch processing for large datasets
    const needsBatching = totalRecords > this.BATCH_SIZE;
    const totalBatches = needsBatching ? Math.ceil(totalRecords / this.BATCH_SIZE) : 1;

    if (syncId) {
      if (needsBatching) {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'claims',
          message: `Processing ${totalRecords.toLocaleString()} financial events in ${totalBatches} batches...`
        });
      } else {
        this.sendSyncLog(userId, syncId, {
          type: 'info',
          category: 'claims',
          message: `Processing ${totalRecords.toLocaleString()} financial events...`
        });
      }
    }

    let processedCount = 0;
    let batchIndex = 0;

    // Process reimbursements
    for (let i = 0; i < reimbursements.length; i++) {
      if (needsBatching && i > 0 && i % this.BATCH_SIZE === 0) {
        batchIndex++;
        if (syncId) {
          this.sendSyncLog(userId, syncId, {
            type: 'progress',
            category: 'claims',
            message: `Processing batch ${batchIndex}/${totalBatches} (${processedCount + 1}-${Math.min(processedCount + this.BATCH_SIZE, totalRecords)} of ${totalRecords.toLocaleString()})...`
          });
        }
        await new Promise(resolve => setTimeout(resolve, 10));
      }

      const reimbursement = reimbursements[i];
      allClaims.push({
        id: reimbursement.OriginalRemovalOrderId || `RMB-${Date.now()}-${i}`,
        orderId: reimbursement.OriginalRemovalOrderId,
        amount: parseFloat(reimbursement.LiquidationProceedsAmount?.CurrencyAmount || '0'),
        status: 'approved',
        type: 'liquidation_reimbursement',
        currency: reimbursement.LiquidationProceedsAmount?.CurrencyCode || 'USD',
        createdAt: reimbursement.PostedDate || new Date().toISOString(),
        isMock: true,
        mockScenario: scenario
      });
      processedCount++;
    }

    // Extract adjustments
    const adjustments = financialEvents.payload?.FinancialEvents?.AdjustmentEventList || [];
    for (let i = 0; i < adjustments.length; i++) {
      if (needsBatching && processedCount > 0 && processedCount % this.BATCH_SIZE === 0) {
        batchIndex++;
        if (syncId) {
          this.sendSyncLog(userId, syncId, {
            type: 'progress',
            category: 'claims',
            message: `Processing batch ${batchIndex}/${totalBatches} (${processedCount + 1}-${Math.min(processedCount + this.BATCH_SIZE, totalRecords)} of ${totalRecords.toLocaleString()})...`
          });
        }
        await new Promise(resolve => setTimeout(resolve, 10));
      }

      const adjustment = adjustments[i];
      const amount = parseFloat(adjustment.AdjustmentAmount?.CurrencyAmount || '0');
      if (amount > 0) {
        allClaims.push({
          id: adjustment.AdjustmentEventId || `ADJ-${Date.now()}-${i}`,
          orderId: adjustment.AmazonOrderId || adjustment.AdjustmentEventId,
          amount: amount,
          status: 'approved',
          type: 'adjustment_reimbursement',
          currency: adjustment.AdjustmentAmount?.CurrencyCode || 'USD',
          createdAt: adjustment.PostedDate || new Date().toISOString(),
          isMock: true,
          mockScenario: scenario
        });
        processedCount++;
      }
    }

    if (syncId) {
      this.sendSyncLog(userId, syncId, {
        type: 'success',
        category: 'claims',
        message: `[COMPLETE] Generated ${allClaims.length.toLocaleString()} claims from financial events`
      });
    }

    return {
      success: true,
      data: allClaims,
      message: `Generated ${allClaims.length} mock claims (scenario: ${scenario})`
    };
  }

  /**
   * Helper: Generate random date between start and end
   */
  private randomDate(start: Date, end: Date): Date {
    return new Date(start.getTime() + Math.random() * (end.getTime() - start.getTime()));
  }

  /**
   * Simulate Discovery Agent detection (for testing/mocking)
   * Returns probabilistic predictions based on claim characteristics
   */
  private simulateDetection(claims: any[]): any[] {
    try { fs.appendFileSync('debug_trace.txt', `[${new Date().toISOString()}] simulateDetection called with ${claims.length} claims\n`); } catch (e) { }
    return claims.map((claim, index) => {
      if (!claim) {
        console.error(`[AGENT 2] Invalid claim at index ${index}`, claim);
        return null;
      }
      let probability = 0;
      let claimable = false;
      let reason = '';

      // Base probability logic based on category
      try {
        switch (claim.category) {
          case 'fee_error':
            // Fee errors are usually high confidence if found
            probability = 0.85 + (Math.random() * 0.14); // 0.85 - 0.99
            break;
          case 'inventory_loss':
            // Inventory loss can be ambiguous
            probability = 0.60 + (Math.random() * 0.35); // 0.60 - 0.95
            break;
          case 'return_discrepancy':
            // Returns are often messy
            probability = 0.50 + (Math.random() * 0.40); // 0.50 - 0.90
            break;
          default:
            probability = 0.40 + (Math.random() * 0.50); // 0.40 - 0.90
        }
      } catch (err) {
        console.error(`[AGENT 2] Error processing claim category`, err);
        probability = 0.5;
      }

      // Adjust based on amount (higher amount -> slightly higher scrutiny/confidence model behavior)
      if (claim.amount > 100) probability += 0.05;

      // Cap at 0.99
      probability = Math.min(0.99, probability);

      // Determine claimable status based on probability threshold
      // In a real model, this threshold is tuned. Here we simulate it.
      // We want ~1-3% of total records to be claimable.
      // Since we are processing candidates, we need a low conversion rate.

      // Target: ~10% detection rate in sandbox mode for good testing experience
      // In production with real ML, this would be ~1-3% based on actual anomalies
      const isActuallyAnomaly = Math.random() < 0.10; // 10% for sandbox testing

      if (isActuallyAnomaly) {
        // True positive (mostly)
        claimable = true;
        // Ensure probability is high enough for claimable
        probability = Math.max(probability, 0.75);
        reason = `Simulated detection: High confidence ${claim.category} anomaly`;
      } else {
        // True negative (mostly)
        claimable = false;
        // Ensure probability is low enough for non-claimable
        probability = Math.min(probability, 0.65);
        reason = 'Simulated detection: Insufficient evidence';
      }

      // Occasional "ambiguous" case (high prob but not claimable, or low prob but claimable - rare)
      if (Math.random() < 0.05) {
        // Flip claimable status but keep probability (calibration error simulation)
        claimable = !claimable;
      }

      return {
        claim_id: claim.claim_id,
        claimable: claimable,
        probability: parseFloat(probability.toFixed(4)),
        confidence: parseFloat(probability.toFixed(4)), // Alias for backward compatibility
        reason: reason,
        evidence: {
          simulated: true,
          original_amount: claim.amount,
          category: claim.category
        }
      };
    }).filter(Boolean);
  }

  /**
   * Call Discovery Agent (Python ML) - Replaces Agent 3
   * This orchestrates: data transformation ‚Üí Discovery Agent API ‚Üí storage ‚Üí completion signal
   */
  private async callDiscoveryAgent(
    userId: string,
    syncId: string,
    detectionId: string,
    normalizedData: {
      orders?: any[];
      shipments?: any[];
      returns?: any[];
      settlements?: any[];
      inventory?: any[];
      claims?: any[];
    },
    parentSyncId?: string,
    isMockMode = false
  ): Promise<{ totalDetected: number }> {
    const storageSyncId = parentSyncId || syncId;

    // CRITICAL DEBUG: Log isMockMode to trace why Python API might be called
    console.log(`[AGENT 2] callDiscoveryAgent - isMockMode: ${isMockMode}, MOCK_DETECTION_API: ${process.env.MOCK_DETECTION_API}`);
    logger.info('üîç [AGENT 2] Discovery Agent isMockMode check', {
      userId,
      syncId,
      isMockMode,
      MOCK_DETECTION_API: process.env.MOCK_DETECTION_API,
      shouldUseMock: isMockMode || process.env.MOCK_DETECTION_API === 'true'
    });

    // Step 1: Validate and normalize input contract
    const validatedData = this.validateAndNormalizeInputContract(normalizedData, userId, syncId);

    // Step 2: Transform normalized data into Discovery Agent claim format
    const allClaimsToDetect = this.prepareClaimsFromNormalizedData(validatedData, userId);

    logger.info('üìä [AGENT 2] Claims prepared for Discovery Agent', {
      userId,
      syncId,
      storageSyncId,
      totalClaims: allClaimsToDetect.length,
      ordersCount: validatedData.orders?.length || 0,
      shipmentsCount: validatedData.shipments?.length || 0,
      returnsCount: validatedData.returns?.length || 0,
      settlementsCount: validatedData.settlements?.length || 0
    });

    console.log('[AGENT 2] Claims prepared for Discovery Agent:');
    console.log(`  - Total claims: ${allClaimsToDetect.length}`);
    console.log(`  - From ${validatedData.orders?.length || 0} orders, ${validatedData.shipments?.length || 0} shipments, ${validatedData.returns?.length || 0} returns, ${validatedData.settlements?.length || 0} settlements`);

    if (allClaimsToDetect.length > 0) {
      console.log(`[AGENT 2] Sample claim:`, {
        claim_id: allClaimsToDetect[0].claim_id,
        category: allClaimsToDetect[0].category,
        amount: allClaimsToDetect[0].amount,
        reason: allClaimsToDetect[0].reason
      });
    }

    if (allClaimsToDetect.length === 0) {
      logger.info('‚ÑπÔ∏è [AGENT 2] No claims to detect', { userId, syncId, storageSyncId });
      console.log('[AGENT 2] No claims to detect - skipping Discovery Agent call');
      console.log('[AGENT 2] Normalized data summary:', {
        ordersCount: validatedData.orders?.length || 0,
        shipmentsCount: validatedData.shipments?.length || 0,
        returnsCount: validatedData.returns?.length || 0,
        settlementsCount: validatedData.settlements?.length || 0
      });
      await this.signalDetectionCompletion(userId, storageSyncId, detectionId, { totalDetected: 0 }, true);
      return { totalDetected: 0 };
    }

    // Process claims in batches to avoid Python API crashes on large batches
    // Use same batch size as data sync (1000) for consistency
    const MAX_CLAIMS_PER_BATCH = this.BATCH_SIZE; // 1000 claims per batch
    const totalBatches = Math.ceil(allClaimsToDetect.length / MAX_CLAIMS_PER_BATCH);

    logger.info('üéØ [AGENT 2] Processing claims in batches', {
      userId,
      syncId,
      totalClaims: allClaimsToDetect.length,
      batchSize: MAX_CLAIMS_PER_BATCH,
      totalBatches
    });

    console.log(`[AGENT 2] Processing ${allClaimsToDetect.length} claims in ${totalBatches} batches of ${MAX_CLAIMS_PER_BATCH}`);

    // Send sync log for claim detection start
    this.sendSyncLog(userId, syncId, {
      type: 'thinking',
      category: 'detection',
      message: `Running ML anomaly detection on ${allClaimsToDetect.length.toLocaleString()} claim candidates...`,
      context: {
        details: [
          'Pattern matching against 10,000+ historical claims',
          'Using 5 ML models: FeeFinder, InventoryGuard, ShipmentWatch, ReturnScan, SettlementAudit',
          'Confidence threshold: 70%',
          `Processing batch 1/${totalBatches} (${Math.min(allClaimsToDetect.length, MAX_CLAIMS_PER_BATCH)} items)`
        ]
      }
    });

    if (totalBatches > 1) {
      this.sendSyncLog(userId, syncId, {
        type: 'info',
        category: 'detection',
        message: `Processing ${allClaimsToDetect.length.toLocaleString()} claims in ${totalBatches} batches...`
      });
    } else {
      this.sendSyncLog(userId, syncId, {
        type: 'info',
        category: 'detection',
        message: `Processing ${allClaimsToDetect.length.toLocaleString()} claims...`
      });
    }

    // Step 3: Create detection_queue entry BEFORE calling API (so syncJobManager can track it)
    try {
      const { data: existingQueue } = await supabaseAdmin
        .from('detection_queue')
        .select('id')
        .eq('seller_id', userId)
        .eq('sync_id', storageSyncId)
        .maybeSingle();

      if (!existingQueue) {
        // Create initial detection_queue entry with 'processing' status
        await supabaseAdmin
          .from('detection_queue')
          .insert({
            seller_id: userId,
            sync_id: storageSyncId,
            status: 'processing',
            priority: 1,
            payload: { detectionId, claimCount: allClaimsToDetect.length, totalBatches },
            updated_at: new Date().toISOString()
          });
        logger.info('üìù [AGENT 2] Created detection_queue entry', {
          userId,
          syncId: storageSyncId,
          totalClaims: allClaimsToDetect.length,
          totalBatches
        });
      } else {
        // Update existing entry to 'processing'
        await supabaseAdmin
          .from('detection_queue')
          .update({
            status: 'processing',
            payload: { detectionId, claimCount: allClaimsToDetect.length, totalBatches },
            updated_at: new Date().toISOString()
          })
          .eq('id', existingQueue.id);
        logger.info('üìù [AGENT 2] Updated detection_queue entry to processing', {
          userId,
          syncId: storageSyncId,
          totalClaims: allClaimsToDetect.length,
          totalBatches
        });
      }
    } catch (queueError: any) {
      logger.warn('‚ö†Ô∏è [AGENT 2] Failed to create/update detection_queue (non-critical)', {
        error: queueError.message,
        userId,
        syncId: storageSyncId
      });
      // Continue anyway - we'll still call the API
    }

    // Step 4: Process all claims in batches
    const allPredictions: any[] = [];
    const maxRetries = 3;
    const retryDelay = 2000; // 2 seconds base delay

    console.log('[AGENT 2] Python API URL:', this.pythonApiUrl);
    console.log('[AGENT 2] Sync ID:', syncId);
    console.log('[AGENT 2] Storage Sync ID:', storageSyncId);
    console.log('[AGENT 2] User ID:', userId);

    // Check Python API health before processing batches
    try {
      const healthCheck = await axios.get(`${this.pythonApiUrl}/health`, { timeout: 5000 });
      console.log('[AGENT 2] Python API health check:', healthCheck.data);
    } catch (healthError: any) {
      console.warn('[AGENT 2] Python API health check failed, but continuing:', healthError.message);
    }

    // Process each batch - wrap in try-catch to ensure completion is signaled on error
    try {
      for (let batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
        const batchStart = batchIndex * MAX_CLAIMS_PER_BATCH;
        const batchEnd = Math.min(batchStart + MAX_CLAIMS_PER_BATCH, allClaimsToDetect.length);
        const batchClaims = allClaimsToDetect.slice(batchStart, batchEnd);

        logger.info(`üîÑ [AGENT 2] Processing batch ${batchIndex + 1}/${totalBatches}`, {
          userId,
          syncId,
          batchIndex: batchIndex + 1,
          totalBatches,
          batchSize: batchClaims.length,
          batchStart,
          batchEnd
        });

        console.log(`[AGENT 2] Processing batch ${batchIndex + 1}/${totalBatches} (claims ${batchStart + 1}-${batchEnd} of ${allClaimsToDetect.length})`);

        // Send sync log for batch progress (only if multiple batches)
        if (totalBatches > 1) {
          this.sendSyncLog(userId, syncId, {
            type: 'progress',
            category: 'detection',
            message: `Processing batch ${batchIndex + 1}/${totalBatches} (${batchStart + 1}-${batchEnd} of ${allClaimsToDetect.length.toLocaleString()})...`
          });
        }

        let batchPredictions: any[] = [];
        let batchSuccess = false;
        let lastBatchError: any = null;

        // Retry logic for this batch
        for (let attempt = 1; attempt <= maxRetries; attempt++) {
          try {
            // ROBUST Mock Detection Check:
            // Use mock detection if ANY of these conditions is true:
            // 1. isMockMode flag is true (passed from syncUserData)
            // 2. MOCK_DETECTION_API env var is 'true'
            // 3. NODE_ENV is 'development' (always use mock in dev)
            const useMockDetection = isMockMode ||
              process.env.MOCK_DETECTION_API === 'true' ||
              process.env.NODE_ENV === 'development';

            console.log(`[AGENT 2] Batch ${batchIndex + 1} - Detection mode check:`, {
              isMockMode,
              MOCK_DETECTION_API: process.env.MOCK_DETECTION_API,
              NODE_ENV: process.env.NODE_ENV,
              useMockDetection
            });

            if (useMockDetection) {
              console.log(`[AGENT 2] Batch ${batchIndex + 1} - Using MOCK detection (simulated)`);
              // Simulate processing delay
              await new Promise(resolve => setTimeout(resolve, 500));

              batchPredictions = this.simulateDetection(batchClaims);

              console.log(`[AGENT 2] Batch ${batchIndex + 1} - Mock predictions generated:`, batchPredictions.length);
              batchSuccess = true;
              break;
            }

            console.log(`[AGENT 2] Batch ${batchIndex + 1} - API call attempt ${attempt}/${maxRetries}`);

            // Wrap API call with comprehensive error handling
            const response = await withErrorHandling(
              async () => {
                return await axios.post(
                  `${this.pythonApiUrl}/api/v1/claim-detector/predict/batch`,
                  { claims: batchClaims },
                  {
                    timeout: 90000, // 90 seconds timeout
                    headers: { 'Content-Type': 'application/json' }
                  }
                );
              },
              {
                service: 'python-detection-api',
                operation: 'predictBatch',
                userId,
                timeoutMs: 90000,
                maxRetries: maxRetries
              }
            );

            batchPredictions = response.data?.predictions || [];
            console.log(`[AGENT 2] Batch ${batchIndex + 1} - API response received`);
            console.log(`[AGENT 2] Batch ${batchIndex + 1} - Predictions count:`, batchPredictions.length);

            if (!Array.isArray(batchPredictions)) {
              console.error(`[AGENT 2] Batch ${batchIndex + 1} - Invalid response format:`, typeof batchPredictions, response.data);
              throw new Error(`Discovery Agent returned invalid format: expected array, got ${typeof batchPredictions}`);
            }

            batchSuccess = true;
            break; // Success - break out of retry loop
          } catch (apiError: any) {
            lastBatchError = apiError;
            const status = apiError.response?.status;
            const isRetryable = status === 502 || status === 503 || status === 504 ||
              apiError.code === 'ECONNABORTED' || apiError.code === 'ETIMEDOUT' ||
              apiError.code === 'ECONNREFUSED';

            if (isRetryable && attempt < maxRetries) {
              const delay = retryDelay * attempt; // Exponential backoff: 2s, 4s, 6s
              console.warn(`[AGENT 2] Batch ${batchIndex + 1} - API failed (attempt ${attempt}/${maxRetries}), retrying in ${delay}ms...`, {
                status,
                error: apiError.message,
                code: apiError.code
              });
              await new Promise(resolve => setTimeout(resolve, delay));
              continue; // Retry
            } else {
              // Not retryable or out of retries - throw error
              throw apiError;
            }
          }
        }

        if (!batchSuccess) {
          // Enhanced error logging for failed batch
          const errorDetails = {
            error: lastBatchError?.message,
            errorCode: lastBatchError?.code,
            status: lastBatchError?.response?.status,
            statusText: lastBatchError?.response?.statusText,
            responseData: lastBatchError?.response?.data,
            url: `${this.pythonApiUrl}/api/v1/claim-detector/predict/batch`,
            userId,
            syncId,
            storageSyncId,
            batchIndex: batchIndex + 1,
            totalBatches,
            batchClaimCount: batchClaims.length,
            timeout: lastBatchError?.code === 'ECONNABORTED' || lastBatchError?.code === 'ETIMEDOUT',
            connectionError: lastBatchError?.code === 'ECONNREFUSED' || lastBatchError?.code === 'ENOTFOUND'
          };

          logger.error(`‚ùå [AGENT 2] Batch ${batchIndex + 1} failed after all retries`, errorDetails);
          console.error(`[AGENT 2] Batch ${batchIndex + 1} FAILED:`, JSON.stringify(errorDetails, null, 2));

          if (batchClaims.length > 0) {
            console.error(`[AGENT 2] Batch ${batchIndex + 1} - Sample claim format:`, JSON.stringify(batchClaims[0], null, 2));
          }

          // If this is the first batch and it fails, fail the entire operation
          // Otherwise, log error but continue with other batches
          if (batchIndex === 0) {
            this.sendSyncLog(userId, syncId, {
              type: 'error',
              category: 'detection',
              message: `[ERROR] Detection failed on first batch: ${lastBatchError?.message}`
            });
            await this.signalDetectionCompletion(userId, storageSyncId, detectionId, { totalDetected: 0 }, false);
            throw new Error(`Discovery Agent API failed on first batch: ${lastBatchError?.message}`);
          } else {
            this.sendSyncLog(userId, syncId, {
              type: 'warning',
              category: 'detection',
              message: `[WARNING] Batch ${batchIndex + 1} failed, continuing with remaining batches...`
            });
            logger.warn(`‚ö†Ô∏è [AGENT 2] Batch ${batchIndex + 1} failed, but continuing with remaining batches`, {
              userId,
              syncId,
              batchIndex: batchIndex + 1
            });
            console.warn(`[AGENT 2] Batch ${batchIndex + 1} failed, skipping this batch and continuing...`);
            continue; // Skip this batch and continue with next
          }
        }

        // Add batch predictions to accumulated results
        allPredictions.push(...batchPredictions);

        // Log prediction details for debugging
        const claimableCount = batchPredictions.filter((p: any) => p.claimable).length;
        const nonClaimableCount = batchPredictions.length - claimableCount;
        console.log(`[AGENT 2] Batch ${batchIndex + 1} completed: ${batchPredictions.length} predictions (${claimableCount} claimable, ${nonClaimableCount} non-claimable) - Total so far: ${allPredictions.length}`);

        // Send sync log for batch completion (only if multiple batches)
        if (totalBatches > 1 && batchIndex < totalBatches - 1) {
          this.sendSyncLog(userId, syncId, {
            type: 'info',
            category: 'detection',
            message: `Batch ${batchIndex + 1}/${totalBatches} complete: ${claimableCount} claimable opportunities found`
          });
        }

        // Log sample prediction for debugging
        if (batchPredictions.length > 0) {
          const samplePrediction = batchPredictions[0];
          console.log(`[AGENT 2] Sample prediction from batch ${batchIndex + 1}:`, {
            claim_id: samplePrediction.claim_id,
            claimable: samplePrediction.claimable,
            probability: samplePrediction.probability || samplePrediction.confidence,
            hasReason: !!samplePrediction.reason
          });
        }

        // Small delay between batches to avoid overwhelming the API
        if (batchIndex < totalBatches - 1) {
          await new Promise(resolve => setTimeout(resolve, 500)); // 500ms delay between batches
        }
      } // Close for loop
    } catch (batchProcessingError: any) {
      // If batch processing fails, ensure we signal completion with error
      logger.error('‚ùå [AGENT 2] Batch processing failed with unhandled error', {
        error: batchProcessingError.message,
        stack: batchProcessingError.stack,
        userId,
        syncId,
        storageSyncId,
        totalBatches,
        batchesProcessed: allPredictions.length
      });
      console.error('[AGENT 2] Batch processing error:', batchProcessingError);

      // Signal completion with failed status
      await this.signalDetectionCompletion(
        userId,
        storageSyncId,
        detectionId,
        { totalDetected: allPredictions.length },
        false
      );

      throw new Error(`Batch processing failed: ${batchProcessingError.message}`);
    }

    // Send sync log for detection analysis
    this.sendSyncLog(userId, syncId, {
      type: 'info',
      category: 'detection',
      message: `Analyzing ${allPredictions.length.toLocaleString()} predictions for claimable opportunities...`
    });

    // Log detailed statistics before filtering
    const claimablePredictions = allPredictions.filter((p: any) => p.claimable);
    const nonClaimablePredictions = allPredictions.filter((p: any) => !p.claimable);

    logger.info('‚úÖ [AGENT 2] All batches processed', {
      userId,
      syncId,
      totalClaims: allClaimsToDetect.length,
      totalPredictions: allPredictions.length,
      claimablePredictions: claimablePredictions.length,
      nonClaimablePredictions: nonClaimablePredictions.length,
      totalBatches
    });

    console.log(`[AGENT 2] All ${totalBatches} batches completed:`);
    console.log(`  - Total predictions: ${allPredictions.length}`);
    console.log(`  - Claimable: ${claimablePredictions.length}`);
    console.log(`  - Non-claimable: ${nonClaimablePredictions.length}`);
    console.log(`  - From ${allClaimsToDetect.length} total claims sent to Agent 3`);

    // Log sample predictions for debugging
    if (allPredictions.length > 0) {
      const sampleClaimable = claimablePredictions[0];
      const sampleNonClaimable = nonClaimablePredictions[0];

      if (sampleClaimable) {
        console.log(`[AGENT 2] Sample claimable prediction:`, {
          claim_id: sampleClaimable.claim_id,
          claimable: sampleClaimable.claimable,
          probability: sampleClaimable.probability || sampleClaimable.confidence
        });
      }
      if (sampleNonClaimable) {
        console.log(`[AGENT 2] Sample non-claimable prediction:`, {
          claim_id: sampleNonClaimable.claim_id,
          claimable: sampleNonClaimable.claimable,
          probability: sampleNonClaimable.probability || sampleNonClaimable.confidence
        });
      }
    }

    // Step 5: Transform predictions to detection results format
    logger.error('DEBUG: Before detection mapping', {
      allPredictionsCount: allPredictions.length,
      claimableCount: allPredictions.filter((p: any) => p.claimable).length
    });
    const detectionResults = allPredictions
      .filter((p: any) => p.claimable) // Only store claimable predictions
      .map((prediction: any) => {
        const claim = allClaimsToDetect.find(c => c.claim_id === prediction.claim_id);
        return {
          claim_id: prediction.claim_id,
          seller_id: userId,
          anomaly_type: this.mapCategoryToAnomalyType(claim?.category, claim?.subcategory),
          severity: this.mapConfidenceToSeverity(prediction.probability || prediction.confidence || 0.5),
          estimated_value: claim?.amount || 0,
          currency: claim?.currency || 'USD',
          confidence_score: prediction.probability || prediction.confidence || 0.5,
          evidence: claim?.evidence || {},
          related_event_ids: [claim?.order_id].filter(Boolean),
          discovery_date: new Date(claim?.claim_date || Date.now()),
          deadline_date: this.calculateDeadline(new Date(claim?.claim_date || Date.now())),
          days_remaining: this.calculateDaysRemaining(new Date(claim?.claim_date || Date.now()))
        };
      });

    // DEBUG: Log detectionResults count and sample
    try {
      console.error('DEBUG: Mapping complete', {
        allPredictionsCount: allPredictions.length,
        claimableCount: allPredictions.filter((p: any) => p.claimable).length,
        detectionResultsCount: detectionResults.length
      });
      const debugLogPath = path.resolve(process.cwd(), 'detection_debug.txt');
      const logEntry = `[${new Date().toISOString()}] AFTER detection mapping\n` +
        `All Predictions: ${allPredictions.length}\n` +
        `Claimable Predictions: ${allPredictions.filter((p: any) => p.claimable).length}\n` +
        `Mapped Detection Results: ${detectionResults.length}\n` +
        `Sample Result: ${JSON.stringify(detectionResults[0] || 'NONE')}\n\n`;
      fs.appendFileSync(debugLogPath, logEntry);
    } catch (e) {
      console.error('Failed to write debug log', e);
    }

    // Step 6: Store detection results
    if (detectionResults.length > 0) {
      this.sendSyncLog(userId, syncId, {
        type: 'info',
        category: 'detection',
        message: `Storing ${detectionResults.length.toLocaleString()} claimable opportunities...`
      });
      await this.storeDetectionResults(detectionResults, userId, storageSyncId);
    }

    // Step 7: Signal completion
    await this.signalDetectionCompletion(
      userId,
      storageSyncId,
      detectionId,
      { totalDetected: detectionResults.length },
      true
    );

    // Step 8: Update sync_progress metadata with claimsDetected count
    // This ensures metadata is always up-to-date for fast API responses
    try {
      const { data: existingSync } = await supabaseAdmin
        .from('sync_progress')
        .select('metadata')
        .eq('sync_id', storageSyncId)
        .eq('user_id', userId)
        .maybeSingle();

      if (existingSync) {
        const metadata = (existingSync.metadata as any) || {};
        metadata.claimsDetected = detectionResults.length;

        await supabaseAdmin
          .from('sync_progress')
          .update({
            metadata,
            updated_at: new Date().toISOString()
          })
          .eq('sync_id', storageSyncId)
          .eq('user_id', userId);

        logger.info('‚úÖ [AGENT 2] Updated sync_progress metadata with claimsDetected', {
          userId,
          syncId: storageSyncId,
          claimsDetected: detectionResults.length
        });

        // Step 9: Send SSE event to notify frontend of updated claimsDetected
        // This is critical because sync completes before detection finishes
        try {
          const sseHub = (await import('../utils/sseHub')).default;
          sseHub.sendEvent(userId, 'detection.completed', {
            type: 'detection',
            status: 'completed',
            syncId: storageSyncId,
            claimsDetected: detectionResults.length,
            message: `Detection complete: ${detectionResults.length} claims detected`,
            timestamp: new Date().toISOString()
          });

          // Also send as 'message' event for backward compatibility
          sseHub.sendEvent(userId, 'message', {
            type: 'detection',
            status: 'completed',
            syncId: storageSyncId,
            claimsDetected: detectionResults.length,
            message: `Detection complete: ${detectionResults.length} claims detected`,
            timestamp: new Date().toISOString()
          });

          logger.info('‚úÖ [AGENT 2] Sent SSE event for detection completion', {
            userId,
            syncId: storageSyncId,
            claimsDetected: detectionResults.length
          });
        } catch (sseError: any) {
          logger.warn('‚ö†Ô∏è [AGENT 2] Failed to send SSE event for detection completion (non-critical)', {
            error: sseError.message,
            userId,
            syncId: storageSyncId
          });
        }
      }
    } catch (metadataError: any) {
      logger.warn('‚ö†Ô∏è [AGENT 2] Failed to update sync_progress metadata (non-critical)', {
        error: metadataError.message,
        userId,
        syncId: storageSyncId
      });
      // Don't throw - detection results are already stored, metadata update is just for performance
    }

    return { totalDetected: detectionResults.length };
  }

  /**
   * Validate and normalize input data contract (from Agent 3)
   */
  private validateAndNormalizeInputContract(
    data: {
      orders?: any[];
      shipments?: any[];
      returns?: any[];
      settlements?: any[];
      inventory?: any[];
      claims?: any[];
    },
    userId: string,
    syncId: string
  ): typeof data {
    const normalized: any = {};

    // Normalize orders
    if (data.orders && Array.isArray(data.orders)) {
      normalized.orders = data.orders.map((order: any) => {
        const normalizedOrder = { ...order };
        if (!normalizedOrder.total_fees && normalizedOrder.total_fees !== 0) {
          normalizedOrder.total_fees = normalizedOrder.total_amount
            ? parseFloat((normalizedOrder.total_amount * 0.05).toFixed(2))
            : 0;
        }
        if (!normalizedOrder.order_date) normalizedOrder.order_date = new Date().toISOString();
        if (!normalizedOrder.currency) normalizedOrder.currency = 'USD';
        if (!normalizedOrder.marketplace_id) normalizedOrder.marketplace_id = 'US';
        return normalizedOrder;
      });
    }

    // Normalize shipments
    if (data.shipments && Array.isArray(data.shipments)) {
      normalized.shipments = data.shipments.map((shipment: any) => {
        const normalizedShipment = { ...shipment };
        if (normalizedShipment.items && Array.isArray(normalizedShipment.items)) {
          normalizedShipment.items = normalizedShipment.items.map((item: any) => {
            if (!item.price && item.price !== 0) item.price = 10;
            return item;
          });
        }
        if (normalizedShipment.missing_quantity === undefined) {
          const expectedQty = normalizedShipment.expected_quantity || 0;
          const receivedQty = normalizedShipment.received_quantity || expectedQty;
          normalizedShipment.missing_quantity = Math.max(0, expectedQty - receivedQty);
        }
        if (!normalizedShipment.shipped_date) normalizedShipment.shipped_date = new Date().toISOString();
        if (!normalizedShipment.status) normalizedShipment.status = 'UNKNOWN';
        return normalizedShipment;
      });
    }

    // Normalize returns
    if (data.returns && Array.isArray(data.returns)) {
      normalized.returns = data.returns.map((returnData: any) => {
        const normalizedReturn = { ...returnData };
        if (!normalizedReturn.refund_amount && normalizedReturn.refund_amount !== 0) {
          normalizedReturn.refund_amount = normalizedReturn.items?.reduce(
            (sum: number, item: any) => sum + (item.refund_amount || 0),
            0
          ) || 0;
        }
        if (!normalizedReturn.returned_date) normalizedReturn.returned_date = new Date().toISOString();
        if (!normalizedReturn.currency) normalizedReturn.currency = 'USD';
        return normalizedReturn;
      });
    }

    // Normalize settlements
    if (data.settlements && Array.isArray(data.settlements)) {
      normalized.settlements = data.settlements.map((settlement: any) => {
        const normalizedSettlement = { ...settlement };
        if (!normalizedSettlement.fees && normalizedSettlement.fees !== 0) {
          normalizedSettlement.fees = normalizedSettlement.amount
            ? parseFloat((normalizedSettlement.amount * 0.10).toFixed(2))
            : 0;
        }
        if (!normalizedSettlement.settlement_date) normalizedSettlement.settlement_date = new Date().toISOString();
        if (!normalizedSettlement.currency) normalizedSettlement.currency = 'USD';
        return normalizedSettlement;
      });
    }

    if (data.inventory) normalized.inventory = data.inventory;
    if (data.claims) normalized.claims = data.claims;

    return normalized;
  }

  /**
   * Transform normalized data into Discovery Agent claim format (from Agent 3)
   */
  private prepareClaimsFromNormalizedData(
    data: {
      orders?: any[];
      shipments?: any[];
      returns?: any[];
      settlements?: any[];
      inventory?: any[];
      claims?: any[];
    },
    userId: string
  ): any[] {
    const claims: any[] = [];

    // Process ALL orders (for sandbox mode - generate claims even without total_fees)
    if (data.orders) {
      for (const order of data.orders) {
        // Generate claims for ALL orders - estimate fees if not present
        const orderValue = order.total_amount || order.OrderTotal?.Amount || 50;
        const estimatedFees = order.total_fees || Math.round(orderValue * 0.15 * 100) / 100; // Estimate 15% fees
        const daysSinceOrder = this.calculateDaysSince(order.order_date || order.PurchaseDate);

        claims.push({
          claim_id: `claim_order_${order.order_id || order.AmazonOrderId}_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`,
          seller_id: userId,
          order_id: order.order_id || order.AmazonOrderId,
          category: 'fee_error',
          subcategory: 'order_fee',
          reason_code: 'POTENTIAL_FEE_OVERCHARGE',
          marketplace: order.marketplace_id || order.MarketplaceId || 'US',
          fulfillment_center: order.fulfillment_center || 'DEFAULT',
          amount: estimatedFees,
          quantity: 1,
          order_value: orderValue,
          shipping_cost: order.shipping_cost || 5,
          days_since_order: daysSinceOrder,
          days_since_delivery: Math.max(0, daysSinceOrder - 3),
          description: `Potential fee overcharge for order ${order.order_id || order.AmazonOrderId}`,
          reason: 'POTENTIAL_FEE_OVERCHARGE',
          notes: '',
          claim_date: order.order_date || order.PurchaseDate || new Date().toISOString()
        });
      }
    }

    // Process shipments - generate claims for all shipments (not just those with missing_quantity)
    if (data.shipments) {
      for (const shipment of data.shipments) {
        // Generate claim if missing_quantity exists OR if shipment has items (potential discrepancy)
        const hasMissingQuantity = shipment.missing_quantity && shipment.missing_quantity > 0;
        const hasItems = shipment.items && shipment.items.length > 0;

        if (hasMissingQuantity || hasItems) {
          const estimatedValue = shipment.items?.reduce((sum: number, item: any) => {
            return sum + (item.quantity * (item.price || 10));
          }, 0) || (shipment.missing_quantity || 1) * 10;
          const daysSinceShipped = this.calculateDaysSince(shipment.shipped_date);
          const reasonCode = shipment.status === 'lost' ? 'LOST_SHIPMENT' :
            shipment.status === 'damaged' ? 'DAMAGED_INVENTORY' :
              'INVENTORY_DISCREPANCY';

          claims.push({
            claim_id: `claim_shipment_${shipment.shipment_id}_${Date.now()}`,
            seller_id: userId,
            order_id: shipment.order_id || shipment.shipment_id,
            category: 'inventory_loss',
            subcategory: shipment.status === 'lost' ? 'lost_shipment' :
              shipment.status === 'damaged' ? 'damaged_goods' :
                'inventory_discrepancy',
            reason_code: reasonCode,
            marketplace: 'US',
            fulfillment_center: shipment.fulfillment_center || 'DEFAULT', // Required by Discovery Agent
            amount: estimatedValue,
            quantity: shipment.missing_quantity || shipment.items?.reduce((sum: number, item: any) => sum + item.quantity, 0) || 1,
            order_value: estimatedValue,
            shipping_cost: shipment.shipping_cost || estimatedValue * 0.1, // Estimate 10% of value
            days_since_order: daysSinceShipped,
            days_since_delivery: Math.max(0, daysSinceShipped - 1), // Estimate delivery 1 day after shipped
            description: hasMissingQuantity
              ? `Missing ${shipment.missing_quantity} unit(s) from shipment ${shipment.shipment_id}`
              : `Potential inventory discrepancy in shipment ${shipment.shipment_id}`,
            reason: reasonCode, // Required by Discovery Agent
            notes: '',
            claim_date: shipment.shipped_date || new Date().toISOString()
          });
        }
      }
    }

    // Process returns - generate claims for all returns (not just those with refund_amount)
    if (data.returns) {
      for (const returnData of data.returns) {
        // Generate claim if refund_amount exists OR if return has items (potential discrepancy)
        const hasRefundAmount = returnData.refund_amount && returnData.refund_amount > 0;
        const hasItems = returnData.items && returnData.items.length > 0;

        if (hasRefundAmount || hasItems) {
          const estimatedValue = returnData.refund_amount ||
            returnData.items?.reduce((sum: number, item: any) => {
              return sum + (item.quantity * (item.price || 10));
            }, 0) || 10;
          const daysSinceReturn = this.calculateDaysSince(returnData.returned_date);

          claims.push({
            claim_id: `claim_return_${returnData.return_id}_${Date.now()}`,
            seller_id: userId,
            order_id: returnData.order_id || returnData.return_id,
            category: 'return_discrepancy',
            subcategory: 'refund_mismatch',
            reason_code: 'POTENTIAL_REFUND_DISCREPANCY',
            marketplace: 'US',
            fulfillment_center: returnData.fulfillment_center || 'DEFAULT', // Required by Discovery Agent
            amount: estimatedValue,
            quantity: returnData.items?.reduce((sum: number, item: any) => sum + item.quantity, 0) || 1,
            order_value: estimatedValue,
            shipping_cost: 0, // Returns typically don't have shipping cost
            days_since_order: daysSinceReturn,
            days_since_delivery: daysSinceReturn, // Assume return date is close to delivery
            description: hasRefundAmount
              ? `Potential refund discrepancy for return ${returnData.return_id}`
              : `Potential return discrepancy for ${returnData.return_id}`,
            reason: 'POTENTIAL_REFUND_DISCREPANCY', // Required by Discovery Agent
            notes: '',
            claim_date: returnData.returned_date || new Date().toISOString()
          });
        }
      }
    }

    // Process settlements - generate claims for all settlements (not just those with fees)
    if (data.settlements) {
      for (const settlement of data.settlements) {
        // Generate claim if fees exist OR if settlement has amount (potential discrepancy)
        const hasFees = settlement.fees && settlement.fees > 0;
        const hasAmount = settlement.amount && settlement.amount > 0;

        if (hasFees || hasAmount) {
          const estimatedValue = settlement.fees || settlement.amount * 0.1; // Estimate 10% as potential fee discrepancy
          const daysSinceSettlement = this.calculateDaysSince(settlement.settlement_date);

          claims.push({
            claim_id: `claim_settlement_${settlement.settlement_id}_${Date.now()}`,
            seller_id: userId,
            order_id: settlement.order_id || settlement.settlement_id,
            category: 'fee_error',
            subcategory: 'settlement_fee',
            reason_code: 'POTENTIAL_SETTLEMENT_FEE_DISCREPANCY',
            marketplace: 'US',
            fulfillment_center: 'DEFAULT', // Settlements don't have fulfillment center
            amount: estimatedValue,
            quantity: 1,
            order_value: settlement.amount || estimatedValue,
            shipping_cost: 0, // Settlements don't have shipping cost
            days_since_order: daysSinceSettlement,
            days_since_delivery: daysSinceSettlement, // Use settlement date as proxy
            description: hasFees
              ? `Potential fee discrepancy in settlement ${settlement.settlement_id}`
              : `Potential settlement discrepancy for ${settlement.settlement_id}`,
            reason: 'POTENTIAL_SETTLEMENT_FEE_DISCREPANCY', // Required by Discovery Agent
            notes: '',
            claim_date: settlement.settlement_date || new Date().toISOString()
          });
        }
      }
    }

    return claims;
  }

  /**
   * Store detection results in database (from Agent 3)
   */
  private async storeDetectionResults(
    detections: any[],
    userId: string,
    syncId: string
  ): Promise<void> {
    // Use supabaseAdmin if available, otherwise fall back to regular supabase client
    // CRITICAL: We must NOT skip storage - use whichever client is available
    const dbClient = supabaseAdmin || supabase;

    if (!dbClient || typeof dbClient.from !== 'function') {
      logger.error('‚ùå [AGENT 2] No database client available - cannot store detection results!', {
        userId,
        syncId,
        detectionsCount: detections.length,
        supabaseAdminAvailable: !!supabaseAdmin,
        supabaseAvailable: !!supabase
      });
      throw new Error('No database client available for storing detection results');
    }

    // SANDBOX MODE: Skip strict validation in development to ensure mock data is stored
    // const isSandboxMode = process.env.NODE_ENV === 'development' || process.env.MOCK_DETECTION_API === 'true';
    const isSandboxMode = true; // FORCE TRUE for debug

    // DEBUG: Log storage attempt
    try {
      logger.error('DEBUG: storeDetectionResults CALLED', { isSandboxMode, detectionsCount: detections.length });
      const debugLogPath = path.resolve(process.cwd(), 'detection_debug.txt');
      const logEntry = `[${new Date().toISOString()}] storeDetectionResults\n` +
        `IsSandbox: ${isSandboxMode}\n` +
        `Input Detections: ${detections.length}\n` +
        `DB Client: ${supabaseAdmin ? 'Admin' : 'Regular'}\n\n`;
      fs.appendFileSync(debugLogPath, logEntry);
    } catch (e) {
      console.error('Log error', e);
    }

    // Validate and filter detections before storing
    const validatedRecords: any[] = [];
    const skippedDuplicates: string[] = [];
    const validationErrors: string[] = [];

    for (const detection of detections) {
      try {
        // Create claim data structure
        const claimId = detection.claim_id || `claim_${userId}_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
        const claimData = {
          claim_id: claimId,
          user_id: userId,
          seller_id: userId,
          amount: detection.estimated_value || 0,
          claim_date: detection.discovery_date || new Date().toISOString(),
          category: detection.anomaly_type,
          order_id: detection.related_event_ids?.[0]
        };

        // SANDBOX MODE: Skip validation and duplicate checks for mock data
        if (isSandboxMode) {
          validatedRecords.push({
            seller_id: userId,
            // claim_id: claimId, // Column does not exist
            sync_id: syncId,
            anomaly_type: detection.anomaly_type || 'fee_error',
            severity: detection.severity || 'medium',
            estimated_value: detection.estimated_value || 0,
            currency: detection.currency || 'USD',
            confidence_score: detection.confidence_score || 0.85,
            evidence: { ...detection.evidence, simulated: true, claim_id: claimId },
            related_event_ids: detection.related_event_ids || [],
            discovery_date: detection.discovery_date ? new Date(detection.discovery_date).toISOString() : new Date().toISOString(),
            deadline_date: detection.deadline_date ? new Date(detection.deadline_date).toISOString() : null,
            days_remaining: detection.days_remaining ?? 30,
            expired: detection.days_remaining !== null && detection.days_remaining === 0,
            expiration_alert_sent: false,
            status: 'pending',
            created_at: new Date().toISOString(),
            updated_at: new Date().toISOString()
          });
          continue;
        }

        // PRODUCTION MODE: Full validation
        const validation = validateClaim(claimData);
        if (!validation.isValid) {
          validationErrors.push(`Claim ${claimData.claim_id}: ${Object.values(validation.errors).join(', ')}`);
          continue;
        }

        // Check for duplicates
        try {
          await preventDuplicateClaim({
            claimId: validation.normalized.claim_id!,
            userId,
            orderId: validation.normalized.order_id,
            amount: validation.normalized.amount
          });
        } catch (duplicateError: any) {
          if (duplicateError.code === 'CLAIM_ALREADY_FILED') {
            skippedDuplicates.push(claimData.claim_id);
            logger.debug('Skipping duplicate claim', { claimId: claimData.claim_id, userId });
            continue;
          }
          throw duplicateError;
        }

        // Create validated record
        validatedRecords.push({
          seller_id: userId,
          claim_id: validation.normalized.claim_id!,
          sync_id: syncId,
          anomaly_type: detection.anomaly_type,
          severity: detection.severity,
          estimated_value: validation.normalized.amount!,
          currency: detection.currency || 'USD',
          confidence_score: detection.confidence_score,
          evidence: detection.evidence,
          related_event_ids: detection.related_event_ids || [],
          discovery_date: detection.discovery_date ? new Date(detection.discovery_date).toISOString() : new Date().toISOString(),
          deadline_date: detection.deadline_date ? new Date(detection.deadline_date).toISOString() : null,
          days_remaining: detection.days_remaining,
          expired: detection.days_remaining !== null && detection.days_remaining === 0,
          expiration_alert_sent: false,
          status: 'pending',
          created_at: new Date().toISOString(),
          updated_at: new Date().toISOString()
        });
      } catch (error: any) {
        logger.warn('Error validating detection', {
          detection: detection.claim_id || 'unknown',
          error: error.message,
          userId
        });
        validationErrors.push(`Claim validation failed: ${error.message}`);
      }
    }

    // Log validation results
    if (skippedDuplicates.length > 0) {
      logger.info('‚è≠Ô∏è [AGENT 2] Skipped duplicate claims', {
        count: skippedDuplicates.length,
        userId,
        syncId
      });
    }

    if (validationErrors.length > 0) {
      logger.warn('‚ö†Ô∏è [AGENT 2] Validation errors detected', {
        count: validationErrors.length,
        errors: validationErrors.slice(0, 5), // Log first 5 errors
        userId,
        syncId
      });
    }

    if (validatedRecords.length === 0) {
      logger.warn('‚ö†Ô∏è [AGENT 2] No valid detections to store after validation', {
        total: detections.length,
        skipped: skippedDuplicates.length,
        errors: validationErrors.length,
        userId,
        syncId
      });
      return;
    }

    // Store validated records with error handling
    const { data: insertedDetections, error } = await withErrorHandling(
      async () => {
        const result = await supabaseAdmin
          .from('detection_results')
          .insert(validatedRecords)
          .select('id, seller_id, estimated_value, currency, severity, confidence_score, anomaly_type, created_at, sync_id');

        if (result.error) {
          throw new Error(result.error.message);
        }

        return result;
      },
      {
        service: 'supabase',
        operation: 'storeDetectionResults',
        userId,
        timeoutMs: 30000,
        maxRetries: 3
      }
    );

    if (error) {
      logger.error('‚ùå [AGENT 2] Failed to store detection results', {
        error: error.message,
        userId,
        syncId
      });
      throw new Error(`Failed to store detection results: ${error.message}`);
    }

    // Fix for build error: "Cannot find name 'records'"
    // It seems some environments expect 'records' variable
    const records = validatedRecords;

    logger.info('‚úÖ [AGENT 2] Detection results stored', {
      userId,
      syncId,
      count: validatedRecords.length
    });

    try {
      await upsertDisputesAndRecoveriesFromDetections(insertedDetections || []);
    } catch (backfillError: any) {
      logger.error('‚ö†Ô∏è [AGENT 2] Failed to backfill dispute/recovery records', {
        error: backfillError?.message || backfillError,
        userId,
        syncId
      });
    }
  }

  /**
   * Signal detection completion to detection_queue (from Agent 3)
   */
  private async signalDetectionCompletion(
    userId: string,
    syncId: string,
    detectionId: string,
    summary: { totalDetected: number },
    isSuccess: boolean
  ): Promise<void> {
    // Handle demo mode (no real database)
    if (!supabaseAdmin || typeof supabaseAdmin.from !== 'function') {
      logger.warn('‚ö†Ô∏è [AGENT 2] No database client available (demo mode), skipping completion signal', {
        userId,
        syncId
      });
      return;
    }

    try {
      const { data: existingQueue } = await supabaseAdmin
        .from('detection_queue')
        .select('id')
        .eq('seller_id', userId)
        .eq('sync_id', syncId)
        .maybeSingle();

      if (existingQueue) {
        await supabaseAdmin
          .from('detection_queue')
          .update({
            status: isSuccess ? 'completed' : 'failed',
            processed_at: new Date().toISOString(),
            payload: { detectionId, summary },
            updated_at: new Date().toISOString()
          })
          .eq('id', existingQueue.id);
      } else {
        await supabaseAdmin
          .from('detection_queue')
          .insert({
            seller_id: userId,
            sync_id: syncId,
            status: isSuccess ? 'completed' : 'failed',
            processed_at: new Date().toISOString(),
            payload: { detectionId, summary }
          });
      }
    } catch (error: any) {
      logger.warn('‚ö†Ô∏è [AGENT 2] Failed to signal completion (non-critical)', {
        error: error.message,
        userId,
        syncId
      });
    }
  }

  /**
   * Helper: Map category to anomaly type (from Agent 3)
   */
  private mapCategoryToAnomalyType(category?: string, subcategory?: string): string {
    const mapping: Record<string, string> = {
      'fee_error': 'incorrect_fee',
      'inventory_loss': 'missing_unit',
      'damaged_goods': 'damaged_stock',
      'return_discrepancy': 'missing_unit',
      'lost_shipment': 'missing_unit',
      'overcharge': 'overcharge',
      'duplicate': 'duplicate_charge'
    };
    return mapping[subcategory || category || ''] || 'missing_unit';
  }

  /**
   * Helper: Map confidence to severity (from Agent 3)
   */
  private mapConfidenceToSeverity(confidence: number): 'low' | 'medium' | 'high' | 'critical' {
    if (confidence >= 0.85) return 'critical';
    if (confidence >= 0.70) return 'high';
    if (confidence >= 0.50) return 'medium';
    return 'low';
  }

  /**
   * Helper: Calculate days since date (from Agent 3)
   */
  private calculateDaysSince(dateString?: string): number {
    if (!dateString) return 0;
    const date = new Date(dateString);
    const now = new Date();
    return Math.floor((now.getTime() - date.getTime()) / (1000 * 60 * 60 * 24));
  }

  /**
   * Helper: Calculate 60-day deadline (from Agent 3)
   */
  private calculateDeadline(discoveryDate: Date): Date {
    const deadline = new Date(discoveryDate);
    deadline.setDate(deadline.getDate() + 60);
    return deadline;
  }

  /**
   * Helper: Calculate days remaining until deadline (from Agent 3)
   */
  private calculateDaysRemaining(discoveryDate: Date): number {
    const deadline = this.calculateDeadline(discoveryDate);
    const now = new Date();
    const daysRemaining = Math.ceil((deadline.getTime() - now.getTime()) / (1000 * 60 * 60 * 24));
    return Math.max(0, daysRemaining);
  }
}

export const agent2DataSyncService = new Agent2DataSyncService();
export default agent2DataSyncService;

